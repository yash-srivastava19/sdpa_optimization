{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu118","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-27T16:03:59.471918Z","iopub.execute_input":"2025-01-27T16:03:59.472170Z","iopub.status.idle":"2025-01-27T16:04:03.896253Z","shell.execute_reply.started":"2025-01-27T16:03:59.472149Z","shell.execute_reply":"2025-01-27T16:04:03.895207Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/nightly/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch.nn.attention.flex_attention import flex_attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T07:52:31.240979Z","iopub.execute_input":"2025-01-20T07:52:31.241390Z","iopub.status.idle":"2025-01-20T07:52:34.809479Z","shell.execute_reply.started":"2025-01-20T07:52:31.241347Z","shell.execute_reply":"2025-01-20T07:52:34.808859Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#Step1: Define the modifying function.\ndef no_op(score, b, h, q_idx, kv_idx):\n    return score\n\n#Step2: Set up the Q,K,V vectors.\nbatch_size = 8\nseq_len = 8\n\nQ = torch.randn(size=(batch_size, seq_len, 128, 128), requires_grad=True, device=\"cuda\")\nK = torch.randn(size=(batch_size, seq_len, 128, 128), requires_grad=True, device=\"cuda\")\nV = torch.randn(size=(batch_size, seq_len, 128, 128), requires_grad=True, device=\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T07:52:42.723540Z","iopub.execute_input":"2025-01-20T07:52:42.723999Z","iopub.status.idle":"2025-01-20T07:52:43.051681Z","shell.execute_reply.started":"2025-01-20T07:52:42.723969Z","shell.execute_reply":"2025-01-20T07:52:43.050808Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"%%file test_benchmark.py\nimport torch\nfrom torch.nn.attention.flex_attention import flex_attention\nfrom functools import lru_cache\n\n#Step1: Define the modifying function.\n@torch.compile \ndef no_op(score, b, h, q_idx, kv_idx):\n    return score\n\n#Step2: Set up the Q,K,V vectors.\nbatch_size = 8\nseq_len = 8\n\nQ = torch.randn(size=(batch_size, seq_len, 128, 128), requires_grad=True, device=\"cuda\")\nK = torch.randn(size=(batch_size, seq_len, 128, 128), requires_grad=True, device=\"cuda\")\nV = torch.randn(size=(batch_size, seq_len, 128, 128), requires_grad=True, device=\"cuda\")\n\n# define the functions as callable.\ndef sdpa():\n  out_sdpa = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n  return out_sdpa\n\ndef flex_sdpa():\n  out_flex = flex_attention(Q, K, V, score_mod=no_op)\n  return out_flex\n\n# make the benchmark functions.\ndef test_torch_sdpa(benchmark):\n  result = benchmark(sdpa)\n\ndef test_flex_attention_no_op(benchmark):\n  result = benchmark(flex_sdpa)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:12:24.633056Z","iopub.execute_input":"2025-01-19T08:12:24.633404Z","iopub.status.idle":"2025-01-19T08:12:24.639876Z","shell.execute_reply.started":"2025-01-19T08:12:24.633381Z","shell.execute_reply":"2025-01-19T08:12:24.638860Z"}},"outputs":[{"name":"stdout","text":"Writing test_benchmark.py\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch\nfrom torch.utils._triton import has_triton\n\nprint(has_triton())  # Without Triton, we can't use the optimizations we want?","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T06:40:00.031207Z","iopub.execute_input":"2025-01-17T06:40:00.031515Z","iopub.status.idle":"2025-01-17T06:40:01.112606Z","shell.execute_reply.started":"2025-01-17T06:40:00.031491Z","shell.execute_reply":"2025-01-17T06:40:01.111610Z"}},"outputs":[{"name":"stdout","text":"False\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"! pytest test_benchmark.py --benchmark-compare","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:12:28.459193Z","iopub.execute_input":"2025-01-19T08:12:28.459526Z","iopub.status.idle":"2025-01-19T08:12:38.719793Z","shell.execute_reply.started":"2025-01-19T08:12:28.459497Z","shell.execute_reply":"2025-01-19T08:12:38.718687Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.10/dist-packages/pytest_benchmark/logger.py:39: PytestBenchmarkWarning: Can't compare. No benchmark files in '/kaggle/working/.benchmarks'. Can't load the previous benchmark.\n  warner(PytestBenchmarkWarning(text))\n\u001b[1m======================================= test session starts ========================================\u001b[0m\nplatform linux -- Python 3.10.12, pytest-8.3.4, pluggy-1.5.0\nbenchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\nrootdir: /kaggle/working\nplugins: benchmark-5.1.0, typeguard-4.4.1, anyio-3.7.1\ncollected 2 items                                                                                  \u001b[0m\u001b[1m\n\ntest_benchmark.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                         [100%]\u001b[0m\n\n\n\u001b[33m----------------------------------------------------------------------------------------------- benchmark: 2 tests -----------------------------------------------------------------------------------------------\u001b[0m\nName (time in us)                    Min                    Max                  Mean              StdDev                Median                 IQR            Outliers          OPS            Rounds  Iterations\n\u001b[33m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\ntest_torch_sdpa             \u001b[32m\u001b[1m     22.2600 (1.0)    \u001b[0m\u001b[32m\u001b[1m      76.0110 (1.0)    \u001b[0m\u001b[32m\u001b[1m     24.3475 (1.0)    \u001b[0m\u001b[32m\u001b[1m    4.5635 (1.0)    \u001b[0m\u001b[32m\u001b[1m     23.1840 (1.0)    \u001b[0m\u001b[32m\u001b[1m    1.4465 (1.0)    \u001b[0m     15;20\u001b[32m\u001b[1m  41,071.9589 (1.0)    \u001b[0m     213           1\ntest_flex_attention_no_op   \u001b[31m\u001b[1m  8,975.9360 (403.23) \u001b[0m\u001b[31m\u001b[1m  10,619.7970 (139.71) \u001b[0m\u001b[31m\u001b[1m  9,481.6756 (389.43) \u001b[0m\u001b[31m\u001b[1m  650.5285 (142.55) \u001b[0m\u001b[31m\u001b[1m  9,246.4540 (398.83) \u001b[0m\u001b[31m\u001b[1m  502.5710 (347.44) \u001b[0m       1;1\u001b[31m\u001b[1m     105.4666 (0.00)   \u001b[0m       5           1\n\u001b[33m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n\nLegend:\n  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\n  OPS: Operations Per Second, computed as 1 / Mean\n\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 7.18s\u001b[0m\u001b[32m =========================================\u001b[0m\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"! pip install pytest-benchmark","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lets define a helpful benchmarking function:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.benchmark as benchmark\ndef benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n    t0 = benchmark.Timer(\n        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n    )\n    return t0.blocked_autorange().mean * 1e6\n\n# Lets define the hyper-parameters of our input\nquery = torch.randn(size=(seq_len, 128, 128), requires_grad=True, device=\"cpu\")\nkey = torch.randn(size=(seq_len, 128, 128), requires_grad=True, device=\"cpu\")\nvalue = torch.randn(size=(seq_len, 128, 128), requires_grad=True, device=\"cpu\")\n\n\nprint(f\"The default implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n\n# Lets explore the speed of each of the 3 implementations\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\n\n\nwith sdpa_kernel(SDPBackend.MATH):\n    math_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, Q, K, V)\n    print(f\"The math implementation runs in {math_time:.3f} microseconds\")\n\nwith sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n    try:\n        flash_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, Q, K, V)\n        print(f\"The flash attention implementation runs in {flash_time:.3f} microseconds\")\n    except RuntimeError as e:\n        print(f\"FlashAttention is not supported. See warnings for reasons: {e}\")\n\nwith sdpa_kernel(SDPBackend.EFFICIENT_ATTENTION):\n    try:\n        efficient_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, Q, K, V)\n        print(f\"The memory efficient implementation runs in {efficient_time:.3f} microseconds\")\n    except RuntimeError:\n        print(\"EfficientAttention is not supported. See warnings for reasons.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.benchmark as benchmark\n\nclass SmolAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.q = torch.nn.Linear(128,128, device=\"cpu\")\n        self.k = torch.nn.Linear(128,128, device=\"cpu\")\n        self.v = torch.nn.Linear(128,128, device=\"cpu\")\n        \n\n    def forward(self, query, key, value):\n        q = self.q(query)\n        k = self.q(key)\n        v = self.q(value)\n        out = q @ k.transpose(-2, -1)\n        probs = F.softmax(out)\n        out = probs @ v\n        return out\n\n\nsa = SmolAttention()\n\nbatch_size = 8\nseq_len = 8\n\nq = torch.randn(size=(seq_len, 128, 128), requires_grad=True, device=\"cpu\")\nk = torch.randn(size=(seq_len, 128, 128), requires_grad=True, device=\"cpu\")\nv = torch.randn(size=(seq_len, 128, 128), requires_grad=True, device=\"cpu\")\n\nsa.eval()\n\nout = sa(q,k,v) # This is working, now need to see how well the quantized model performs\n# print(out.dtype) # fp32\n\n\nsa_q = torch.ao.quantization.quantize_dynamic(\n    sa,  # the original model\n    {nn.Linear, nn.Linear, nn.Linear},  # a set of layers to dynamically quantize\n    dtype=torch.qint8)  # the target dtype for quantized weights\n\n# run the model. This works!! The values are close, but not enough s\nout_q = sa_q(q,k,v)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T07:53:22.445697Z","iopub.execute_input":"2025-01-20T07:53:22.446068Z","iopub.status.idle":"2025-01-20T07:53:23.376771Z","shell.execute_reply.started":"2025-01-20T07:53:22.446038Z","shell.execute_reply":"2025-01-20T07:53:23.375635Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-5-431a5828804b>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  probs = F.softmax(out)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## What Next?\n\nBefore moving forward, let's have a brief recap:\n\n1. FlexAttention is up and working. The `torch.compile` flex attention is not working. I suppose this is because we don't have triton, so we can't write optimized kernels. If we had a local cude machine, I would've narrowed down the error and give conclusive answer.\n\n2. I didn't make use of block_mask in FlexAttention as sparsity is not a bottleneck I've explored as of now. Might do in future.\n\n3. Apart from all these, I got to know the `F.scaled_dot_product_attention`, for CUDA tensor inputs makes use of 3 different techniques, namely: memory efficient attention, Flash attention, and a native C++ implementation. When this function is called, the best performing version is used. We can also isolate all these 3 methods(ptrblck to the rescue once again) and use any one of the specific methods.\n\n4. Making quantization work was a bit of a hassle. PTDQ for now supports only `Linear` and `Recurrent` layers, so directly using `MultiheadAttention` was not working. A workaround was to implement the SDPA with Q,K,V as linear layers, and it seemed to be working(PTDQ, Eager mode)\n\n\nNow, let us think what is something we can do given both of these axes are up and running.\n\n1. Understand the KV Cache mechanism in Pytorch and how can we implement it with SDPA.\n2. Running a lot of benchmark tests. If we are able to get all these 3(Quantization, FlexAttn, KV Cache) together with SDPA, out problem reduces to a search problem which we can optimize. In order to optimize, we need to run a lot of benchmark tests. Learn about Pytorch profiler, it'll come in real handy.\n3. Start thinking about the API of the package if the benchmark tests are giving some results.","metadata":{}},{"cell_type":"code","source":"## KV Cache. Claude generated.\n\n### v2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CachedSDPA(nn.Module):\n    def __init__(self, max_seq_len, head_dim):\n        super().__init__()\n        self.max_seq_len = max_seq_len\n        self.head_dim = head_dim\n        self.cache_k = None\n        self.cache_v = None\n        self.cur_len = 0\n    \n    def forward(self, q, k, v, is_causal=True):\n        # q, k, v: [batch, heads, seq_len, head_dim]\n        \n        # Handle incremental state\n        if self.cache_k is not None:\n            k = torch.cat([self.cache_k, k], dim=2)\n            v = torch.cat([self.cache_v, v], dim=2)\n        \n        # Update cache\n        self.cache_k = k\n        self.cache_v = v\n        self.cur_len = k.shape[2]\n        \n        # Use PyTorch's native SDPA with incremental state\n        out = F.scaled_dot_product_attention(\n            q, k, v,\n            attn_mask=None,  # PyTorch handles causal mask internally when is_causal=True\n            dropout_p=0.0,\n            is_causal=is_causal\n        )\n        \n        return out\n\n    def reset_cache(self):\n        self.cache_k = None\n        self.cache_v = None\n        self.cur_len = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:31:09.992165Z","iopub.execute_input":"2025-01-20T09:31:09.992495Z","iopub.status.idle":"2025-01-20T09:31:09.998709Z","shell.execute_reply.started":"2025-01-20T09:31:09.992465Z","shell.execute_reply":"2025-01-20T09:31:09.997843Z"}},"outputs":[],"execution_count":111},{"cell_type":"markdown","source":"Let's get to profiling. We'll see how things pan out.","metadata":{}},{"cell_type":"code","source":"from torch.profiler import profile, record_function, ProfilerActivity ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T07:53:47.692018Z","iopub.execute_input":"2025-01-20T07:53:47.692307Z","iopub.status.idle":"2025-01-20T07:53:47.696049Z","shell.execute_reply.started":"2025-01-20T07:53:47.692284Z","shell.execute_reply":"2025-01-20T07:53:47.695039Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# test with quantized and non-quantized models.\nwith profile(activities=[ProfilerActivity.CPU], record_shapes=True, profile_memory=True) as prof1:\n    with record_function(\"model_inference\"):\n        sa(q,k,v)\n\n\nwith profile(activities=[ProfilerActivity.CPU], record_shapes=True, profile_memory=True) as prof2:\n    with record_function(\"model_inference\"):\n        sa_q(q,k,v)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T07:54:13.127670Z","iopub.execute_input":"2025-01-20T07:54:13.128036Z","iopub.status.idle":"2025-01-20T07:54:13.194693Z","shell.execute_reply.started":"2025-01-20T07:54:13.128006Z","shell.execute_reply":"2025-01-20T07:54:13.193734Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-5-431a5828804b>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  probs = F.softmax(out)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(\"For Non-Quantized Model:\")\nprint(prof1.key_averages().table(sort_by=\"cpu_time_total\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T07:54:16.405883Z","iopub.execute_input":"2025-01-20T07:54:16.406186Z","iopub.status.idle":"2025-01-20T07:54:16.415639Z","shell.execute_reply.started":"2025-01-20T07:54:16.406161Z","shell.execute_reply":"2025-01-20T07:54:16.414827Z"}},"outputs":[{"name":"stdout","text":"For Non-Quantized Model:\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n         model_inference        40.31%       6.145ms       100.00%      15.245ms      15.245ms           0 b      -3.00 Mb             1  \n            aten::linear        22.69%       3.459ms        40.99%       6.249ms       2.083ms       1.50 Mb           0 b             3  \n             aten::addmm        12.35%       1.883ms        17.42%       2.656ms     885.465us       1.50 Mb       1.50 Mb             3  \n            aten::matmul         0.29%      44.463us        15.21%       2.319ms       1.159ms       1.00 Mb           0 b             2  \n           aten::reshape         8.72%       1.330ms         9.14%       1.393ms     199.020us           0 b           0 b             7  \n               aten::bmm         5.86%     893.907us         5.87%     894.875us     447.437us       1.00 Mb       1.00 Mb             2  \n             aten::copy_         4.96%     755.536us         4.96%     755.536us     251.845us           0 b           0 b             3  \n           aten::softmax         0.06%       9.173us         3.43%     522.559us     522.559us     512.00 Kb           0 b             1  \n          aten::_softmax         3.37%     513.386us         3.37%     513.386us     513.386us     512.00 Kb     512.00 Kb             1  \n              aten::view         0.57%      87.502us         0.57%      87.502us       9.722us           0 b           0 b             9  \n            aten::expand         0.26%      39.074us         0.30%      46.365us       6.624us           0 b           0 b             7  \n                 aten::t         0.14%      21.770us         0.29%      44.656us      14.885us           0 b           0 b             3  \n         aten::transpose         0.14%      22.069us         0.22%      33.084us       8.271us           0 b           0 b             4  \n        aten::as_strided         0.12%      18.306us         0.12%      18.306us       1.664us           0 b           0 b            11  \n      aten::_unsafe_view         0.07%      10.485us         0.07%      10.485us       5.243us           0 b           0 b             2  \n    aten::_reshape_alias         0.06%       9.262us         0.06%       9.262us       9.262us           0 b           0 b             1  \n      aten::resolve_conj         0.02%       3.423us         0.02%       3.423us       0.342us           0 b           0 b            10  \n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 15.245ms\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(\"For Quantized Model:\")\nprint(prof2.key_averages().table(sort_by=\"cpu_time_total\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T07:17:17.023459Z","iopub.execute_input":"2025-01-18T07:17:17.023731Z","iopub.status.idle":"2025-01-18T07:17:17.032457Z","shell.execute_reply.started":"2025-01-18T07:17:17.023710Z","shell.execute_reply":"2025-01-18T07:17:17.031612Z"}},"outputs":[{"name":"stdout","text":"For Quantized Model:\n-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n              model_inference        14.15%     859.224us       100.00%       6.073ms       6.073ms           0 b      -3.00 Mb             1  \n    quantized::linear_dynamic        44.86%       2.724ms        46.98%       2.853ms     951.002us       1.50 Mb      -1.50 Mb             3  \n                 aten::matmul         0.97%      59.103us        28.70%       1.743ms     871.582us       1.00 Mb           0 b             2  \n                    aten::bmm        26.21%       1.592ms        26.23%       1.593ms     796.580us       1.00 Mb       1.00 Mb             2  \n                aten::softmax         0.10%       6.368us         9.78%     593.762us     593.762us     512.00 Kb           0 b             1  \n               aten::_softmax         9.67%     587.394us         9.67%     587.394us     587.394us     512.00 Kb     512.00 Kb             1  \n                  aten::empty         1.82%     110.691us         1.82%     110.691us      18.448us       3.00 Mb       3.00 Mb             6  \n             aten::empty_like         0.29%      17.826us         0.79%      48.231us      16.077us       1.50 Mb           0 b             3  \n                aten::reshape         0.31%      18.873us         0.65%      39.624us       9.906us           0 b           0 b             4  \n                 aten::expand         0.50%      30.132us         0.59%      35.866us       8.967us           0 b           0 b             4  \n              aten::transpose         0.27%      16.359us         0.33%      20.223us      20.223us           0 b           0 b             1  \n           aten::_unsafe_view         0.25%      15.412us         0.25%      15.412us       7.706us           0 b           0 b             2  \n                   aten::view         0.24%      14.447us         0.24%      14.447us       4.816us           0 b           0 b             3  \n             aten::as_strided         0.16%       9.598us         0.16%       9.598us       1.920us           0 b           0 b             5  \n         aten::_reshape_alias         0.10%       6.304us         0.10%       6.304us       6.304us           0 b           0 b             1  \n                     aten::to         0.06%       3.872us         0.06%       3.872us       1.291us           0 b           0 b             3  \n           aten::resolve_conj         0.02%       1.386us         0.02%       1.386us       0.346us           0 b           0 b             4  \n-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 6.073ms\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"with profile(activities=[ProfilerActivity.CUDA], profile_memory=True) as prof3:\n    with record_function(\"model_inference\"):\n        m(Q,K,V)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:11:49.536003Z","iopub.execute_input":"2025-01-18T12:11:49.536274Z","iopub.status.idle":"2025-01-18T12:11:49.546942Z","shell.execute_reply.started":"2025-01-18T12:11:49.536254Z","shell.execute_reply":"2025-01-18T12:11:49.546062Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"print(\"For KVCached Model:\")\nprint(prof3.key_averages().table(sort_by=\"cpu_time_total\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with profile(activities=[ProfilerActivity.CUDA], record_shapes=True, profile_memory=True) as prof4:\n    with record_function(\"model_inference\"):\n        flex_attention(Q, K, V, score_mod=no_op)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T07:22:34.488288Z","iopub.execute_input":"2025-01-18T07:22:34.488632Z","iopub.status.idle":"2025-01-18T07:22:35.724611Z","shell.execute_reply.started":"2025-01-18T07:22:34.488609Z","shell.execute_reply":"2025-01-18T07:22:35.723977Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"print(\"For Flex Attention Model:\")\nprint(prof4.key_averages().table(sort_by=\"cuda_time_total\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I am able to do profiling for all the models, now I just need to figure out how to make graphs and such, or if Pytorch provides a experimentation framework so that we can get to a more detailed study.","metadata":{}},{"cell_type":"code","source":"prof1.export_chrome_trace(\"trace1.json\")  # we can also export it to json and see it in chrome.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T07:31:15.925322Z","iopub.execute_input":"2025-01-18T07:31:15.925777Z","iopub.status.idle":"2025-01-18T07:31:15.930950Z","shell.execute_reply.started":"2025-01-18T07:31:15.925742Z","shell.execute_reply":"2025-01-18T07:31:15.930026Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"Now, we'll figure out how to run benchmarking(for timing, not memory). We can use Pytorch's built in utils benchmarking facility. \n\nThat is good, but let me figure things out using the module `pytorch-benchmark`\n\nUpdate: pytorch-bencmark does not contain the things I want, so I'll not be using that.","metadata":{}},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom typing import List, Dict\nfrom dataclasses import dataclass\n\n@dataclass\nclass ProfilingResult:\n    \"\"\"Store profiling data for multiple runs\"\"\"\n    name: str\n    avg_time: float\n    cpu_time: float\n    #cuda_time: float\n    cpu_memory: float\n    #cuda_memory: float\n\nclass ProfileAnalyzer:\n    def __init__(self):\n        self.runs: Dict[str, List[ProfilingResult]] = {}\n    \n    def analyze_profile(self, prof: torch.profiler.profile, run_name: str = \"default\"):\n        \"\"\"Extract key metrics from profiler output\"\"\"\n        if run_name not in self.runs:\n            self.runs[run_name] = []\n            \n        for event in prof.key_averages():\n            result = ProfilingResult(\n                name=event.key,\n                avg_time=event.cpu_time_total / 1000,  # convert to ms\n                cpu_time=event.cpu_time_total / 1000,\n                #cuda_time=event.cuda_time_total / 1000 if event.cuda_time_total else 0,\n                cpu_memory=event.cpu_memory_usage / 1024 / 1024,  # convert to MB\n                #cuda_memory=event.cuda_memory_usage / 1024 / 1024 if event.cuda_memory_usage else 0\n            )\n            self.runs[run_name].append(result)\n    \n    def plot_comparison(self, metric: str = \"avg_time\", top_k: int = 20):\n        \"\"\"Plot comparison of specified metric across runs\"\"\"\n        plt.figure(figsize=(12, 6))\n        \n        # Convert data to DataFrame for easier plotting\n        data = []\n        for run_name, results in self.runs.items():\n            for result in results:\n                data.append({\n                    \"run\": run_name,\n                    \"operation\": result.name,\n                    metric: getattr(result, metric)\n                })\n        \n        df = pd.DataFrame(data)\n        \n        # Get top k operations by total time across all runs\n        top_ops = df.groupby(\"operation\")[metric].sum().nlargest(top_k).index\n        df_filtered = df[df[\"operation\"].isin(top_ops)]\n        \n        # Plot\n        #df_pivot = df_filtered.pivot(index=\"operation\", columns=\"run\", values=metric)\n        df_filtered.plot(kind=\"bar\", ax=plt.gca())\n        \n        plt.title(f\"Top {top_k} Operations by {metric}\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        return plt.gcf()\n\n# Example usage and bottleneck analysis\ndef analyze_bottlenecks(model, q, k, v, batch_size=32):\n    \"\"\"Example of identifying and analyzing bottlenecks\"\"\"\n    analyzer = ProfileAnalyzer()\n    \n    # Profile original model\n    with torch.profiler.profile(\n        activities=[\n            torch.profiler.ProfilerActivity.CPU,\n            torch.profiler.ProfilerActivity.CUDA,\n        ],\n        record_shapes=True,\n        with_stack=False\n    ) as prof:\n        output = model(q, k, v)\n    \n    analyzer.analyze_profile(prof, \"original\")\n    \n    # Example optimization: Add batch norm fusion\n    model.eval()\n    with torch.profiler.profile(\n        activities=[\n            torch.profiler.ProfilerActivity.CPU,\n            torch.profiler.ProfilerActivity.CUDA,\n        ],\n        record_shapes=True,\n        with_stack=False\n    ) as prof:\n        with record_function(\"model_inference\"):\n            output = model(q, k, v)\n            \n    analyzer.analyze_profile(prof, \"optimized\")\n    \n    # Plot comparisons\n    analyzer.plot_comparison(metric=\"avg_time\", top_k=5)\n    return analyzer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:46:55.683262Z","iopub.execute_input":"2025-01-18T12:46:55.683558Z","iopub.status.idle":"2025-01-18T12:46:55.694270Z","shell.execute_reply.started":"2025-01-18T12:46:55.683535Z","shell.execute_reply":"2025-01-18T12:46:55.693528Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# Profile your model\nanalyzer = analyze_bottlenecks(m, Q,K,V)\n\n# Check most time-consuming operations\nanalyzer.plot_comparison(metric=\"avg_time\") \n# Shows top operations by time\n#analyzer.plot_comparison(metric=\"cuda_memory\")  # Shows memory usage","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Claude Generated\n\nimport torch\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict\nfrom dataclasses import dataclass\n\n@dataclass\nclass OperationMetrics:\n    \"\"\"Detailed metrics for a single operation\"\"\"\n    name: str\n    self_cpu_time: float  # ms\n    cpu_memory: float     # MB\n    cuda_time: float      # ms\n    cuda_memory: float    # MB\n    calls: int\n    input_shapes: List[str]\n    stack_context: str\n\nclass PerformanceAnalyzer:\n    def __init__(self):\n        self.operations: Dict[str, List[OperationMetrics]] = {}\n    \n    def analyze_profile(self, prof: torch.profiler.profile, run_name: str = \"default\"):\n        \"\"\"Extract detailed performance metrics with context\"\"\"\n        self.operations[run_name] = []\n        \n        for event in prof.key_averages():\n            # Get stack trace for context\n            stack = event.stack if event.stack else []\n            stack_context = \"\\n\".join(str(frame) for frame in stack[-3:])  # Last 3 frames\n            \n            metrics = OperationMetrics(\n                name=event.key,\n                self_cpu_time=event.self_cpu_time_total / 1000,\n                cpu_memory=event.cpu_memory_usage / 1024 / 1024,\n                cuda_time=event.cuda_time_total / 1000 if event.cuda_time_total else 0,\n                cuda_memory=event.cuda_memory_usage / 1024 / 1024 if event.cuda_memory_usage else 0,\n                calls=event.count,\n                input_shapes=[str(shape) for shape in event.input_shapes],\n                stack_context=stack_context\n            )\n            self.operations[run_name].append(metrics)\n\n    def plot_bottleneck_analysis(self, run_name: str = \"default\"):\n        \"\"\"Create actionable visualization of performance bottlenecks\"\"\"\n        ops = self.operations[run_name]\n        \n        # Create figure with subplots\n        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n        \n        # 1. Time per call analysis\n        df_time = pd.DataFrame([{\n            'Operation': op.name,\n            'CPU Time/Call': op.self_cpu_time / op.calls,\n            'CUDA Time/Call': op.cuda_time / op.calls,\n            'Calls': op.calls\n        } for op in ops])\n        \n        # Sort by total time per call\n        df_time['Total Time/Call'] = df_time['CPU Time/Call'] + df_time['CUDA Time/Call']\n        df_time = df_time.nlargest(10, 'Total Time/Call')\n        \n        # Plot time distribution\n        df_time.plot(kind='barh', x='Operation', \n                    y=['CPU Time/Call', 'CUDA Time/Call'], \n                    ax=ax1, stacked=True)\n        \n        # Add call count annotations\n        for i, calls in enumerate(df_time['Calls']):\n            ax1.text(df_time['Total Time/Call'].max() * 1.05, i, \n                    f'Calls: {calls}', va='center')\n        \n        ax1.set_title('Top 10 Time-Consuming Operations (per call)')\n        ax1.set_xlabel('Time (ms)')\n        \n        # 2. Memory impact visualization\n        df_mem = pd.DataFrame([{\n            'Operation': op.name,\n            'CPU Memory (MB)': op.cpu_memory,\n            'CUDA Memory (MB)': op.cuda_memory,\n            'Input Shapes': '\\n'.join(op.input_shapes[:2])  # Show first 2 shapes\n        } for op in ops])\n        \n        df_mem['Total Memory'] = df_mem['CPU Memory (MB)'] + df_mem['CUDA Memory (MB)']\n        df_mem = df_mem.nlargest(10, 'Total Memory')\n        \n        # Plot memory usage\n        df_mem.plot(kind='barh', x='Operation', \n                   y=['CPU Memory (MB)', 'CUDA Memory (MB)'], \n                   ax=ax2, stacked=True)\n        \n        # Add input shape annotations\n        for i, shapes in enumerate(df_mem['Input Shapes']):\n            ax2.text(df_mem['Total Memory'].max() * 1.05, i, \n                    f'Shapes: {shapes}', va='center')\n        \n        ax2.set_title('Top 10 Memory-Intensive Operations')\n        ax2.set_xlabel('Memory (MB)')\n        \n        plt.tight_layout()\n        return fig\n\n    def get_bottleneck_report(self, run_name: str = \"default\") -> str:\n        \"\"\"Generate actionable report of potential bottlenecks\"\"\"\n        ops = self.operations[run_name]\n        report = []\n        \n        # Find operations with high time/call ratio\n        time_heavy = sorted(ops, \n                          key=lambda x: (x.self_cpu_time + x.cuda_time) / x.calls, \n                          reverse=True)[:5]\n        \n        report.append(\"Top 5 Time-Intensive Operations (per call):\")\n        for op in time_heavy:\n            report.append(f\"\\n{op.name}:\")\n            report.append(f\"- Time per call: {(op.self_cpu_time + op.cuda_time)/op.calls:.2f}ms\")\n            report.append(f\"- Called {op.calls} times\")\n            report.append(f\"- Input shapes: {', '.join(op.input_shapes[:2])}\")\n            if op.stack_context:\n                report.append(f\"- Context: {op.stack_context}\")\n        \n        return \"\\n\".join(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:49:24.701078Z","iopub.execute_input":"2025-01-18T12:49:24.701379Z","iopub.status.idle":"2025-01-18T12:49:24.714721Z","shell.execute_reply.started":"2025-01-18T12:49:24.701353Z","shell.execute_reply":"2025-01-18T12:49:24.713912Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"analyzer = PerformanceAnalyzer()\n\n# Profile your model\nwith torch.profiler.profile(...) as prof:\n    output = model(input)\n\nanalyzer.analyze_profile(prof, \"baseline\")\n\n# Get visual and textual analysis\nanalyzer.plot_bottleneck_analysis(\"baseline\")\nprint(analyzer.get_bottleneck_report(\"baseline\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we'll try to see Pytorch benchmark and see how it pans. The timeit module there can do multiple things, and we need to explore the thread facility in it :)","metadata":{}},{"cell_type":"code","source":"import torch.utils.benchmark as benchmark","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T07:55:05.815314Z","iopub.execute_input":"2025-01-20T07:55:05.815606Z","iopub.status.idle":"2025-01-20T07:55:05.819147Z","shell.execute_reply.started":"2025-01-20T07:55:05.815585Z","shell.execute_reply":"2025-01-20T07:55:05.818247Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"num_threads = torch.get_num_threads()\nnum_threads","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T07:55:19.075380Z","iopub.execute_input":"2025-01-20T07:55:19.075658Z","iopub.status.idle":"2025-01-20T07:55:19.081136Z","shell.execute_reply.started":"2025-01-20T07:55:19.075636Z","shell.execute_reply":"2025-01-20T07:55:19.080456Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"t0 = benchmark.Timer(\n    stmt='flex_attention(Q,K,V, score_mod=no_op)',\n    setup = \"from __main__ import flex_attention, no_op\",\n    globals={'Q':Q, 'K': K, 'V':V},\n    num_threads=num_threads,\n    label = \"Multithreaded SDPA - FlexAttention\"\n)\n\nprint(f\"{t0.timeit(100)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T07:55:28.471134Z","iopub.execute_input":"2025-01-20T07:55:28.471410Z","iopub.status.idle":"2025-01-20T07:55:32.221102Z","shell.execute_reply.started":"2025-01-20T07:55:28.471390Z","shell.execute_reply":"2025-01-20T07:55:32.220289Z"}},"outputs":[{"name":"stdout","text":"<torch.utils.benchmark.utils.common.Measurement object at 0x7a00a2430c40>\nMultithreaded SDPA - FlexAttention\nsetup: from __main__ import flex_attention, no_op\n  8.17 ms\n  1 measurement, 100 runs , 2 threads\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"## Pyorch code to compare benchmark performances:\n## Will use later if required.\nfrom itertools import product\n\n# Compare takes a list of measurements which we'll save in results.\nresults = []\n\nsizes = [1, 64, 1024, 10000]\nfor b, n in product(sizes, sizes):\n    # label and sub_label are the rows\n    # description is the column\n    label = 'Batched dot'\n    sub_label = f'[{b}, {n}]'\n    x = torch.ones((b, n))\n    for num_threads in [1, 4, 16, 32]:\n        results.append(benchmark.Timer(\n            stmt='batched_dot_mul_sum(x, x)',\n            setup='from __main__ import batched_dot_mul_sum',\n            globals={'x': x},\n            num_threads=num_threads,\n            label=label,\n            sub_label=sub_label,\n            description='mul/sum',\n        ).blocked_autorange(min_run_time=1))\n        results.append(benchmark.Timer(\n            stmt='batched_dot_bmm(x, x)',\n            setup='from __main__ import batched_dot_bmm',\n            globals={'x': x},\n            num_threads=num_threads,\n            label=label,\n            sub_label=sub_label,\n            description='bmm',\n        ).blocked_autorange(min_run_time=1))\n\ncompare = benchmark.Compare(results)\ncompare.print()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Updates\n\nThings are going and can go well depending of the next steps. Till now,\n\n1. Able run profilers on different attention variants.\n2. Run benchmarks on different variants, with option to compare different benchmarks too.\n3. Able to run code on different threads, and note performance gains.\n4. Have a custom library for graphing and understanding memeory and speed throughput.\n\nPytorch is really wonderful.\n\nNow, just writing a testing suite is required. After that, only comparision is left.\n\nWorklog soon!!","metadata":{}},{"cell_type":"code","source":"## There is real alpha in this.\nimport torch.utils.benchmark as benchmark\nfrom torch.profiler import profile, record_function, ProfilerActivity \n\nclass PerformanceAnalysis:\n    def __init__(self, func, inputs, *args):\n        self.m = func # this could be a nn.Module, function, or anything else.\n        self.q, self.k, self.v = inputs #unpack  the inputs, we need it in this format only.\n        self.args = args\n        self.setup()\n        \n    def setup(self):\n        pass \n\n    def profile(self):\n        # Update, can add multiple settings.\n        if self.q.device == \"cuda\":\n            act = ProfilerActivity.CUDA\n        else:\n            act = ProfilerActivity.CPU\n        \n        with profile(activities=[act], record_shapes=True, profile_memory=True) as prof4:\n            with record_function(\"model_inference\"):\n                if self.args:\n                    self.m(self.q, self.k, self.v, self.args[0])\n                else:\n                    self.m(self.q, self.k, self.v)\n        \n        return prof4.key_averages().table()\n    \n    def benchmark(self, use_threads=True, num_exprs=100):\n        # Custom Logic to make the statement for Benchmark Timer\n\n        # If we figure out the class/function, setup part is done. Just need to fire out how the Q,K,V names are made.\n        # Update: No need. We did it lol.\n        # Update: Stuck on the benchmark class/function thing. \n        # Update: Made it work after scraping lol.\n        \n        import inspect\n        if inspect.isfunction(self.m):\n            func_name = f\"{self.m.__name__}\"\n            # print(func_name)\n            name = func_name\n            if self.args:\n                module_name = f\"{self.args[0].__name__}\"\n                stmt_str = f\"M(Q, K, V, {module_name})\"\n                setup_str = f\"from __main__ import {module_name}\"\n            else:\n                stmt_str = f\"M(Q, K, V)\"\n                setup_str = f\"from __main__ import {func_name}\"\n        else: # it must be a class. Add checks, but for now we can proceed.\n            class_name = f\"{self.m.__class__.__name__}\"\n            name = class_name\n            if self.args:\n                module_name = f\"{self.args[0]}\"\n                stmt_str = f\"M(Q, K, V, {module_name})\"\n                setup_str = f\"from __main__ import {class_name}, {module_name}\"\n            else:\n                stmt_str = f\"M(Q, K, V)\"\n                setup_str = f\"from __main__ import {class_name}\"\n\n        if use_threads:\n            # Sorted\n            num_threads = torch.get_num_threads()\n            t0 = benchmark.Timer(\n                stmt = stmt_str,\n                setup = setup_str,\n                globals={'M':self.m, 'Q':self.q, 'K': self.k, 'V':self.v},\n                num_threads=num_threads,\n                label = f\"Multi Threaded SDPA - {name}\"\n            )\n        else:\n            t0 = benchmark.Timer(\n                stmt = stmt_str,\n                setup = setup_str,\n                globals={'M': self.m, 'Q':self.q, 'K': self.k, 'V':self.v},\n                label = f\"Single Threaded SDPA - {name}\"\n            )\n\n        return t0.timeit(num_exprs)\n        \n\n    def run(self):\n        pass \n\n    def report(self):\n        return f\"\"\"\n=======================================================================================================================\nPerformance Analysis - Memory and Benchmark Report:\n=======================================================================================================================\n***********************************************************************************************************************\nMemory Profile Report:\n***********************************************************************************************************************\n\n{self.profile()}\n        \n***********************************************************************************************************************\nBenchmark Profile Report:\n***********************************************************************************************************************\n\n{self.benchmark()}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T16:05:20.145803Z","iopub.execute_input":"2025-01-27T16:05:20.146113Z","iopub.status.idle":"2025-01-27T16:05:22.491157Z","shell.execute_reply.started":"2025-01-27T16:05:20.146086Z","shell.execute_reply":"2025-01-27T16:05:22.490283Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"pa = PerformanceAnalysis(flex_attention, (Q, K, V), no_op)\n\nprint(pa.report())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Updates:\n\n**Note:** The thing with pytorch built in benchmark is that it is just cumbersome. You write strings and such and it has a confusing API where we can easily run into scoping problems.\n\n**Update:** After one initial ray of brilliance, I'm able to get the pytorch built in benchmark compatible with my class. It is going to be easy, a LOT!! \n \nDon't wanna procrastinate on writing tests and such. We should just get to it, and save the results in some file idk. Just to have a sanity check, I'll start off by running profiler and benchmark for:\n\n1. KVCache SDPA\n2. Quantized/Non Quantized SDPA. \n3. Flex Attention with different score modifiers.\n\nHow should we go about it? \nFor reproducability, set the seed for Pytorch.\n\nIn a single cell, write the test for each variant -> Write the cell into a file -> Pytest.\n\nIn the cell, model+inputs -> profiler -> benchmarker -> return/print results. ","metadata":{}},{"cell_type":"markdown","source":"# KV Cache Tests\n\nThis work slaps ngl. The work we did to make the `PerformanceAnalysis` class reduced a lot of boilerplate, and the overhead from this was reduced a lot. We just need to make it more general to that we can pass on hyperparameters and can test with more options, but first, we had to make it work.\n\n**Note:** We are running frequently into CUDA out of memory errors, so stay in the in zone till we figure out ways to deal with it.","metadata":{}},{"cell_type":"code","source":"## Actual Definition:\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CachedSDPA(nn.Module):\n    def __init__(self, max_seq_len, head_dim):\n        super().__init__()\n        self.max_seq_len = max_seq_len\n        self.head_dim = head_dim\n        self.cache_k = None\n        self.cache_v = None\n        self.cur_len = 0\n    \n    def forward(self, q, k, v, is_causal=True):\n        # q, k, v: [batch, heads, seq_len, head_dim]\n        \n        # Handle incremental state\n        if self.cache_k is not None:\n            k = torch.cat([self.cache_k, k], dim=1)\n            v = torch.cat([self.cache_v, v], dim=1)\n        \n        # Update cache\n        self.cache_k = k\n        self.cache_v = v\n        self.cur_len = k.shape[2]\n        \n        # Use PyTorch's native SDPA with incremental state\n        out = F.scaled_dot_product_attention(\n            q, k, v,\n            attn_mask=None,  # PyTorch handles causal mask internally when is_causal=True\n            dropout_p=0.0,\n            is_causal=is_causal\n        )\n        \n        return out\n\n    def reset_cache(self):\n        self.cache_k = None\n        self.cache_v = None\n        self.cur_len = 0\n\n# Now, these functions on their own won't make too much sense. Changing hyperparams, and similar things will actually be nice.\ndef test_cached_sdpa_memory():\n    pa.profile()\n\ndef test_cached_sdpa_benchmark():\n    pa.benchmark()\n\nseq_lengths = [4, 8, 10, 12] \nhidden_dims = [16, 32, 64, 128]\n\n# We are running into CUDA out of memory erros frequently, so stay in the required zone.\n\nprint(\"*********************** Cached SDPA Tests *************************\")\n\nfor i, (sl, hd) in enumerate(zip(seq_lengths, hidden_dims)):\n    print(f\"=============== Experiment {i+1}: seq_len={sl}, hidden_dim={hd} =========================\")\n    q = torch.randn(size=(sl, hd, hd), requires_grad=True, device=\"cuda\")\n    k = torch.randn(size=(sl, hd, hd), requires_grad=True, device=\"cuda\")\n    v = torch.randn(size=(sl, hd, hd), requires_grad=True, device=\"cuda\")\n\n    m = CachedSDPA(sl, hd)\n\n    pa = PerformanceAnalysis(m, (q,k,v))\n    print(\"=============== Memory Profile: ===========================\")\n    print(pa.profile())\n    \n    print(\"=============== Benchmark Report: =========================\")\n    print(pa.benchmark())\n\n    torch.cuda.empty_cache()\n    del q,k,v,m,pa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T16:05:35.586319Z","iopub.execute_input":"2025-01-27T16:05:35.586767Z","iopub.status.idle":"2025-01-27T16:05:37.329200Z","shell.execute_reply.started":"2025-01-27T16:05:35.586739Z","shell.execute_reply":"2025-01-27T16:05:37.328412Z"}},"outputs":[{"name":"stdout","text":"*********************** Cached SDPA Tests *************************\n=============== Experiment 1: seq_len=4, hidden_dim=16 =========================\n=============== Memory Profile: ===========================\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                        model_inference         1.69%       5.734ms       100.00%     338.528ms     338.528ms           0 b           0 b             1  \n                     aten::scaled_dot_product_attention         3.19%      10.803ms        98.31%     332.793ms     332.793ms           0 b           0 b             1  \n               aten::_scaled_dot_product_attention_math         3.85%      13.026ms        95.11%     321.990ms     321.990ms           0 b           0 b             1  \n                                              aten::mul         3.50%      11.849ms         8.65%      29.298ms      14.649ms           0 b           0 b             2  \n                                       cudaLaunchKernel        50.40%     170.607ms        50.40%     170.607ms      10.036ms           0 b           0 b            17  \n                                             aten::ones         0.56%       1.899ms         5.33%      18.051ms      18.051ms           0 b           0 b             1  \n                                            aten::empty         0.05%     174.089us         0.05%     174.089us      21.761us           0 b           0 b             8  \n                                            aten::fill_         1.00%       3.379ms         4.82%      16.319ms       3.264ms           0 b           0 b             5  \n                                             aten::tril         1.95%       6.611ms         5.37%      18.172ms      18.172ms           0 b           0 b             1  \n                                    aten::scalar_tensor         0.31%       1.053ms         0.40%       1.363ms     340.764us           0 b           0 b             4  \n                                      aten::logical_not         1.24%       4.183ms        13.83%      46.821ms      23.411ms           0 b           0 b             2  \n                                          aten::resize_         0.03%      97.112us         0.03%      97.112us      32.371us           0 b           0 b             3  \n                                            aten::where         1.07%       3.609ms        15.03%      50.881ms      16.960ms           0 b           0 b             3  \n                                               aten::to         0.00%       3.246us         0.00%       3.246us       1.082us           0 b           0 b             3  \n                                        aten::transpose         0.47%       1.600ms         0.48%       1.615ms       1.615ms           0 b           0 b             1  \n                                       aten::as_strided         0.01%      28.524us         0.01%      28.524us       5.705us           0 b           0 b             5  \n                                           aten::matmul         1.43%       4.837ms        26.37%      89.275ms      44.637ms           0 b           0 b             2  \n                                           aten::expand         0.81%       2.746ms         0.82%       2.759ms     689.787us           0 b           0 b             4  \n                                          aten::reshape         0.23%     795.432us         0.43%       1.460ms     365.053us           0 b           0 b             4  \n                                             aten::view         0.19%     651.874us         0.19%     651.874us     217.291us           0 b           0 b             3  \n                                   aten::_reshape_alias         0.00%      12.905us         0.00%      12.905us      12.905us           0 b           0 b             1  \n                                              aten::bmm         7.30%      24.702ms        23.10%      78.184ms      39.092ms           0 b           0 b             2  \n                                               cudaFree        11.52%      38.987ms        11.52%      38.987ms      19.493ms           0 b           0 b             2  \n                                 cudaDeviceGetAttribute         0.00%       8.928us         0.00%       8.928us       0.595us           0 b           0 b            15  \n                                   cudaGetSymbolAddress         0.40%       1.348ms         0.40%       1.348ms       1.348ms           0 b           0 b             1  \n                                             cudaMalloc         0.20%     675.537us         0.20%     675.537us     168.884us           0 b           0 b             4  \n                                  cudaStreamIsCapturing         0.00%       3.031us         0.00%       3.031us       3.031us           0 b           0 b             1  \ncudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         3.67%      12.420ms         3.67%      12.420ms       2.484ms           0 b           0 b             5  \n                                     aten::_unsafe_view         0.60%       2.035ms         0.60%       2.035ms       1.017ms           0 b           0 b             2  \n                                             aten::add_         0.82%       2.784ms         6.58%      22.290ms      22.290ms           0 b           0 b             1  \n                                    aten::_safe_softmax         0.04%     141.258us        23.43%      79.329ms      79.329ms           0 b           0 b             1  \n                                          aten::softmax         0.52%       1.748ms         7.55%      25.574ms      25.574ms           0 b           0 b             1  \n                                         aten::_softmax         0.52%       1.745ms         7.04%      23.826ms      23.826ms           0 b           0 b             1  \n                                               aten::eq         0.87%       2.949ms         7.54%      25.529ms      25.529ms           0 b           0 b             1  \n                                              aten::all         1.56%       5.281ms         8.22%      27.827ms      27.827ms           0 b           0 b             1  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 338.528ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e60004e62c0>\nMulti Threaded SDPA - CachedSDPA\nsetup: from __main__ import CachedSDPA\n  504.01 us\n  1 measurement, 100 runs , 2 threads\n=============== Experiment 2: seq_len=8, hidden_dim=32 =========================\n=============== Memory Profile: ===========================\n-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                  model_inference         1.42%     227.670us       100.00%      16.041ms      16.041ms           0 b           0 b             1  \n               aten::scaled_dot_product_attention        24.44%       3.921ms        98.58%      15.813ms      15.813ms           0 b           0 b             1  \n         aten::_scaled_dot_product_attention_math         0.33%      52.345us        74.14%      11.892ms      11.892ms           0 b           0 b             1  \n                                        aten::mul         0.61%      97.748us         0.73%     117.007us      58.503us           0 b           0 b             2  \n                                 cudaLaunchKernel         0.75%     119.900us         0.75%     119.900us       7.053us           0 b           0 b            17  \n                                       aten::ones         0.04%       6.664us         0.17%      28.004us      28.004us           0 b           0 b             1  \n                                      aten::empty         0.26%      41.861us         0.26%      41.861us       5.233us           0 b           0 b             8  \n                                      aten::fill_         0.19%      31.100us         0.34%      55.148us      11.030us           0 b           0 b             5  \n                                       aten::tril         0.10%      16.508us         0.14%      21.814us      21.814us           0 b           0 b             1  \n                              aten::scalar_tensor         0.10%      16.684us         0.49%      78.813us      19.703us           0 b           0 b             4  \n                                aten::logical_not         0.09%      13.890us         0.29%      46.273us      23.136us           0 b           0 b             2  \n                                    aten::resize_         0.07%      11.923us         0.07%      11.923us       3.974us           0 b           0 b             3  \n                                      aten::where         0.23%      37.122us         0.68%     109.322us      36.441us           0 b           0 b             3  \n                                         aten::to         0.01%       1.253us         0.01%       1.253us       0.418us           0 b           0 b             3  \n                                  aten::transpose         0.06%      10.115us         0.08%      12.216us      12.216us           0 b           0 b             1  \n                                 aten::as_strided         0.03%       5.250us         0.03%       5.250us       1.050us           0 b           0 b             5  \n                                     aten::matmul         0.27%      42.889us        70.62%      11.329ms       5.664ms           0 b           0 b             2  \n                                     aten::expand         0.12%      20.022us         0.14%      23.171us       5.793us           0 b           0 b             4  \n                                    aten::reshape         0.07%      11.166us         0.15%      24.592us       6.148us           0 b           0 b             4  \n                                       aten::view         0.06%      10.250us         0.06%      10.250us       3.417us           0 b           0 b             3  \n                             aten::_reshape_alias         0.02%       3.176us         0.02%       3.176us       3.176us           0 b           0 b             1  \n                                        aten::bmm        14.67%       2.354ms        69.99%      11.227ms       5.613ms           0 b           0 b             2  \n    cudaOccupancyMaxActiveBlocksPerMultiprocessor        55.17%       8.849ms        55.17%       8.849ms     737.456us           0 b           0 b            12  \n                               aten::_unsafe_view         0.07%      11.032us         0.07%      11.032us       5.516us           0 b           0 b             2  \n                                       aten::add_         0.33%      53.205us         0.42%      67.118us      67.118us           0 b           0 b             1  \n                              aten::_safe_softmax         0.15%      23.972us         1.02%     164.073us     164.073us           0 b           0 b             1  \n                                    aten::softmax         0.02%       3.683us         0.18%      28.170us      28.170us           0 b           0 b             1  \n                                   aten::_softmax         0.10%      16.557us         0.15%      24.487us      24.487us           0 b           0 b             1  \n                                         aten::eq         0.09%      14.307us         0.12%      19.092us      19.092us           0 b           0 b             1  \n                                        aten::all         0.10%      16.575us         0.14%      22.323us      22.323us           0 b           0 b             1  \n-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 16.041ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e60011c1060>\nMulti Threaded SDPA - CachedSDPA\nsetup: from __main__ import CachedSDPA\n  567.59 us\n  1 measurement, 100 runs , 2 threads\n=============== Experiment 3: seq_len=10, hidden_dim=64 =========================\n=============== Memory Profile: ===========================\n-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                  model_inference         4.39%     192.678us       100.00%       4.388ms       4.388ms           0 b           0 b             1  \n               aten::scaled_dot_product_attention        78.65%       3.451ms        95.61%       4.195ms       4.195ms           0 b           0 b             1  \n         aten::_scaled_dot_product_attention_math         1.08%      47.591us        16.95%     743.864us     743.864us           0 b           0 b             1  \n                                        aten::mul         2.26%      99.040us         2.74%     120.086us      60.043us           0 b           0 b             2  \n                                 cudaLaunchKernel         2.68%     117.601us         2.68%     117.601us       6.918us           0 b           0 b            17  \n                                       aten::ones         0.15%       6.633us         0.65%      28.569us      28.569us           0 b           0 b             1  \n                                      aten::empty         1.01%      44.527us         1.01%      44.527us       5.566us           0 b           0 b             8  \n                                      aten::fill_         0.74%      32.651us         1.32%      57.751us      11.550us           0 b           0 b             5  \n                                       aten::tril         1.05%      46.113us         1.28%      56.027us      56.027us           0 b           0 b             1  \n                              aten::scalar_tensor         0.38%      16.684us         1.90%      83.566us      20.891us           0 b           0 b             4  \n                                aten::logical_not         0.33%      14.565us         1.11%      48.828us      24.414us           0 b           0 b             2  \n                                    aten::resize_         0.29%      12.768us         0.29%      12.768us       4.256us           0 b           0 b             3  \n                                      aten::where         0.86%      37.947us         2.56%     112.194us      37.398us           0 b           0 b             3  \n                                         aten::to         0.03%       1.202us         0.03%       1.202us       0.401us           0 b           0 b             3  \n                                  aten::transpose         0.25%      10.774us         0.29%      12.825us      12.825us           0 b           0 b             1  \n                                 aten::as_strided         0.12%       5.375us         0.12%       5.375us       1.075us           0 b           0 b             5  \n                                     aten::matmul         0.66%      28.918us         4.54%     199.313us      99.657us           0 b           0 b             2  \n                                     aten::expand         0.43%      18.743us         0.50%      22.067us       5.517us           0 b           0 b             4  \n                                    aten::reshape         0.25%      10.825us         0.53%      23.184us       5.796us           0 b           0 b             4  \n                                       aten::view         0.21%       9.231us         0.21%       9.231us       3.077us           0 b           0 b             3  \n                             aten::_reshape_alias         0.07%       3.128us         0.07%       3.128us       3.128us           0 b           0 b             1  \n                                        aten::bmm         1.88%      82.426us         2.70%     118.425us      59.212us           0 b           0 b             2  \n    cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.26%      11.482us         0.26%      11.482us       0.957us           0 b           0 b            12  \n                               aten::_unsafe_view         0.15%       6.719us         0.15%       6.719us       3.359us           0 b           0 b             2  \n                                       aten::add_         0.36%      15.591us         0.50%      21.759us      21.759us           0 b           0 b             1  \n                              aten::_safe_softmax         0.47%      20.683us         3.36%     147.235us     147.235us           0 b           0 b             1  \n                                    aten::softmax         0.07%       3.036us         0.46%      20.256us      20.256us           0 b           0 b             1  \n                                   aten::_softmax         0.28%      12.189us         0.39%      17.220us      17.220us           0 b           0 b             1  \n                                         aten::eq         0.29%      12.537us         0.41%      17.896us      17.896us           0 b           0 b             1  \n                                        aten::all         0.34%      14.885us         0.46%      20.266us      20.266us           0 b           0 b             1  \n-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 4.388ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e5f68ada470>\nMulti Threaded SDPA - CachedSDPA\nsetup: from __main__ import CachedSDPA\n  1.60 ms\n  1 measurement, 100 runs , 2 threads\n=============== Experiment 4: seq_len=12, hidden_dim=128 =========================\n=============== Memory Profile: ===========================\n-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                  model_inference         3.87%     188.068us       100.00%       4.859ms       4.859ms           0 b           0 b             1  \n               aten::scaled_dot_product_attention        70.85%       3.443ms        96.13%       4.671ms       4.671ms           0 b           0 b             1  \n         aten::_scaled_dot_product_attention_math         1.11%      54.160us        25.28%       1.228ms       1.228ms           0 b           0 b             1  \n                                        aten::mul         2.55%     124.124us         6.63%     321.999us     161.000us           0 b           0 b             2  \n                                 cudaLaunchKernel         2.54%     123.181us         2.54%     123.181us       7.246us           0 b           0 b            17  \n                                       aten::ones         0.14%       6.956us         0.68%      32.846us      32.846us           0 b           0 b             1  \n                                      aten::empty         0.95%      46.169us         0.95%      46.169us       5.771us           0 b           0 b             8  \n                                      aten::fill_         0.73%      35.422us         1.26%      61.124us      12.225us           0 b           0 b             5  \n                                       aten::tril         0.34%      16.429us         0.46%      22.461us      22.461us           0 b           0 b             1  \n                              aten::scalar_tensor         0.40%      19.529us         1.80%      87.666us      21.916us           0 b           0 b             4  \n                                aten::logical_not         0.35%      16.908us         1.09%      52.888us      26.444us           0 b           0 b             2  \n                                    aten::resize_         0.29%      13.884us         0.29%      13.884us       4.628us           0 b           0 b             3  \n                                      aten::where         0.84%      40.940us         2.42%     117.793us      39.264us           0 b           0 b             3  \n                                         aten::to         0.02%       1.192us         0.02%       1.192us       0.397us           0 b           0 b             3  \n                                  aten::transpose         0.24%      11.861us         0.30%      14.380us      14.380us           0 b           0 b             1  \n                                 aten::as_strided         0.17%       8.221us         0.17%       8.221us       1.644us           0 b           0 b             5  \n                            cudaStreamIsCapturing         0.05%       2.473us         0.05%       2.473us       1.236us           0 b           0 b             2  \n                                       cudaMalloc         7.21%     350.562us         7.21%     350.562us     175.281us           0 b           0 b             2  \n                                     aten::matmul         0.69%      33.477us         4.69%     227.831us     113.916us           0 b           0 b             2  \n                                     aten::expand         0.48%      23.447us         0.60%      29.149us       7.287us           0 b           0 b             4  \n                                    aten::reshape         0.24%      11.798us         0.54%      26.189us       6.547us           0 b           0 b             4  \n                                       aten::view         0.22%      10.579us         0.22%      10.579us       3.526us           0 b           0 b             3  \n                             aten::_reshape_alias         0.08%       3.812us         0.08%       3.812us       3.812us           0 b           0 b             1  \n                                        aten::bmm         2.13%     103.556us         2.72%     131.956us      65.978us           0 b           0 b             2  \n    cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.26%      12.560us         0.26%      12.560us       1.047us           0 b           0 b            12  \n                               aten::_unsafe_view         0.15%       7.060us         0.15%       7.060us       3.530us           0 b           0 b             2  \n                                       aten::add_         0.44%      21.188us         0.59%      28.672us      28.672us           0 b           0 b             1  \n                              aten::_safe_softmax         0.67%      32.552us         8.55%     415.257us     415.257us           0 b           0 b             1  \n                                    aten::softmax         0.11%       5.365us         4.58%     222.671us     222.671us           0 b           0 b             1  \n                                   aten::_softmax         0.53%      25.543us         4.47%     217.306us     217.306us           0 b           0 b             1  \n                                         aten::eq         0.92%      44.740us         1.11%      54.159us      54.159us           0 b           0 b             1  \n                                        aten::all         0.42%      20.448us         0.56%      27.291us      27.291us           0 b           0 b             1  \n-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 4.859ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e5f68ada080>\nMulti Threaded SDPA - CachedSDPA\nsetup: from __main__ import CachedSDPA\n  4.96 ms\n  1 measurement, 100 runs , 2 threads\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Quantized/Non Quantized Tests\n\nIf we were able to do the previous test, this will be ez af.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.benchmark as benchmark\n\nclass SmolAttention(nn.Module):\n    def __init__(self, max_seq_len, hidden_dims):\n        super().__init__()\n        self.q = nn.Linear(hidden_dims, hidden_dims)\n        self.k = nn.Linear(hidden_dims, hidden_dims)\n        self.v = nn.Linear(hidden_dims, hidden_dims)\n\n    def forward(self, query, key, value):\n        q = self.q(query)\n        k = self.k(query)\n        v = self.v(query)\n        return F.scaled_dot_product_attention(q, k, v)\n\nseq_lengths = [4, 8, 10, 12] \nhidden_dims = [16, 32, 64, 128]\n\n\nprint(\"*********************** Non - Quantized SDPA Tests *************************\")\n\nfor i, (sl, hd) in enumerate(zip(seq_lengths, hidden_dims)):\n    print(f\"=============== Experiment {i+1}: seq_len={sl}, hidden_dim={hd} =========================\")\n    q = torch.randn(size=(sl, hd, hd), requires_grad=True, device=\"cpu\")\n    k = torch.randn(size=(sl, hd, hd), requires_grad=True, device=\"cpu\")\n    v = torch.randn(size=(sl, hd, hd), requires_grad=True, device=\"cpu\")\n\n    m = SmolAttention(sl, hd)\n    m_q = torch.ao.quantization.quantize_dynamic(m,{nn.Linear, nn.Linear, nn.Linear},dtype=torch.qint8)\n\n    print(\"Non-Quantized Model: \")\n    pa = PerformanceAnalysis(m, (q,k,v))\n    print(\"=============== Memory Profile: ===========================\")\n    print(pa.profile())\n    \n    print(\"=============== Benchmark Report: =========================\")\n    print(pa.benchmark())\n    \n    print(\"Quantized Model: \")\n    pa = PerformanceAnalysis(m_q, (q,k,v))\n    print(\"=============== Memory Profile: ===========================\")\n    print(pa.profile())\n    \n    print(\"=============== Benchmark Report: =========================\")\n    print(pa.benchmark())\n\n    del q,k,v,m,m_q,pa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T16:37:49.316349Z","iopub.execute_input":"2025-01-27T16:37:49.316687Z","iopub.status.idle":"2025-01-27T16:37:50.345751Z","shell.execute_reply.started":"2025-01-27T16:37:49.316658Z","shell.execute_reply":"2025-01-27T16:37:50.344994Z"}},"outputs":[{"name":"stdout","text":"*********************** Non - Quantized SDPA Tests *************************\n=============== Experiment 1: seq_len=4, hidden_dim=16 =========================\nNon-Quantized Model: \n=============== Memory Profile: ===========================\n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             model_inference        17.26%       1.217ms       100.00%       7.047ms       7.047ms           0 b     -28.00 Kb             1  \n                                aten::linear         2.24%     157.620us        76.38%       5.382ms       1.794ms      12.00 Kb           0 b             3  \n                               aten::reshape         0.43%      30.023us         1.10%      77.865us      11.124us           0 b           0 b             7  \n                                  aten::view         0.82%      57.598us         0.82%      57.598us       6.400us           0 b           0 b             9  \n                                     aten::t         0.32%      22.758us         0.66%      46.416us      15.472us           0 b           0 b             3  \n                             aten::transpose         0.38%      26.885us         0.51%      36.177us       9.044us           0 b           0 b             4  \n                            aten::as_strided         0.24%      16.887us         0.24%      16.887us       1.535us           0 b           0 b            11  \n                                 aten::addmm        72.11%       5.082ms        72.69%       5.122ms       1.707ms      12.00 Kb      12.00 Kb             3  \n                                aten::expand         0.57%      40.187us         0.68%      47.782us       6.826us           0 b           0 b             7  \n                                 aten::copy_         0.37%      26.196us         0.37%      26.196us       5.239us           0 b           0 b             5  \n                          aten::resolve_conj         0.04%       2.704us         0.04%       2.704us       0.270us           0 b           0 b            10  \n          aten::scaled_dot_product_attention         0.30%      20.846us         6.36%     448.140us     448.140us      16.00 Kb           0 b             1  \n    aten::_scaled_dot_product_attention_math         0.39%      27.605us         6.06%     427.294us     427.294us      16.00 Kb      -4.00 Kb             1  \n                                   aten::mul         0.60%      42.408us         1.04%      73.319us      36.659us       8.00 Kb       7.99 Kb             2  \n                                    aten::to         0.10%       7.059us         0.45%      31.574us       7.893us           8 b           0 b             4  \n                              aten::_to_copy         0.20%      14.284us         0.35%      24.515us      12.258us           8 b           0 b             2  \n                         aten::empty_strided         0.07%       4.794us         0.07%       4.794us       2.397us           8 b           8 b             2  \n                                aten::matmul         0.56%      39.416us         2.76%     194.388us      97.194us       8.00 Kb           0 b             2  \n                        aten::_reshape_alias         0.11%       7.979us         0.11%       7.979us       7.979us           0 b           0 b             1  \n                                   aten::bmm         1.04%      73.487us         1.06%      74.412us      37.206us       8.00 Kb       8.00 Kb             2  \n                          aten::_unsafe_view         0.16%      11.063us         0.16%      11.063us       5.532us           0 b           0 b             2  \n                         aten::_safe_softmax         0.45%      31.724us         1.69%     118.800us     118.800us       4.00 Kb      -5.07 Kb             1  \n                               aten::softmax         0.08%       5.351us         0.43%      30.420us      30.420us       4.00 Kb           0 b             1  \n                              aten::_softmax         0.36%      25.069us         0.36%      25.069us      25.069us       4.00 Kb       4.00 Kb             1  \n                         aten::scalar_tensor         0.08%       5.973us         0.08%       5.973us       2.986us           8 b           8 b             2  \n                                    aten::eq         0.12%       8.618us         0.12%       8.618us       8.618us       1.00 Kb       1.00 Kb             1  \n                                   aten::all         0.25%      17.653us         0.31%      21.809us      21.809us          64 b          64 b             1  \n                                 aten::fill_         0.06%       4.156us         0.06%       4.156us       4.156us           0 b           0 b             1  \n                                 aten::where         0.27%      18.798us         0.29%      20.256us      20.256us       4.00 Kb       4.00 Kb             1  \n                                 aten::empty         0.02%       1.458us         0.02%       1.458us       1.458us           0 b           0 b             1  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 7.047ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e5f21ccebc0>\nMulti Threaded SDPA - SmolAttention\nsetup: from __main__ import SmolAttention\n  243.24 us\n  1 measurement, 100 runs , 2 threads\nQuantized Model: \n=============== Memory Profile: ===========================\n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             model_inference        41.69%       6.475ms       100.00%      15.533ms      15.533ms           0 b     -28.00 Kb             1  \n                   quantized::linear_dynamic        55.78%       8.664ms        56.02%       8.702ms       2.901ms      12.00 Kb     -12.00 Kb             3  \n                                 aten::empty         0.16%      25.463us         0.16%      25.463us       3.638us      24.00 Kb      24.00 Kb             7  \n                            aten::empty_like         0.09%      13.549us         0.11%      17.856us       5.952us      12.00 Kb           0 b             3  \n                                    aten::to         0.05%       7.101us         0.22%      33.669us       4.810us           8 b           0 b             7  \n          aten::scaled_dot_product_attention         0.04%       6.574us         2.28%     354.333us     354.333us      16.00 Kb           0 b             1  \n    aten::_scaled_dot_product_attention_math         0.14%      21.007us         2.24%     347.759us     347.759us      16.00 Kb      -4.00 Kb             1  \n                                   aten::mul         0.27%      41.316us         0.47%      72.565us      36.283us       8.00 Kb       7.99 Kb             2  \n                              aten::_to_copy         0.08%      12.697us         0.17%      26.568us      13.284us           8 b           0 b             2  \n                         aten::empty_strided         0.03%       4.633us         0.03%       4.633us       2.316us           8 b           8 b             2  \n                                 aten::copy_         0.06%       9.238us         0.06%       9.238us       4.619us           0 b           0 b             2  \n                             aten::transpose         0.07%      10.178us         0.08%      12.781us      12.781us           0 b           0 b             1  \n                            aten::as_strided         0.04%       6.266us         0.04%       6.266us       1.253us           0 b           0 b             5  \n                                aten::matmul         0.21%      32.846us         1.03%     160.075us      80.038us       8.00 Kb           0 b             2  \n                                aten::expand         0.23%      35.868us         0.25%      39.531us       9.883us           0 b           0 b             4  \n                               aten::reshape         0.09%      14.231us         0.20%      30.930us       7.732us           0 b           0 b             4  \n                                  aten::view         0.08%      12.079us         0.08%      12.079us       4.026us           0 b           0 b             3  \n                        aten::_reshape_alias         0.03%       4.620us         0.03%       4.620us       4.620us           0 b           0 b             1  \n                                   aten::bmm         0.32%      48.949us         0.32%      49.695us      24.847us       8.00 Kb       8.00 Kb             2  \n                          aten::resolve_conj         0.00%       0.746us         0.00%       0.746us       0.187us           0 b           0 b             4  \n                          aten::_unsafe_view         0.05%       7.073us         0.05%       7.073us       3.536us           0 b           0 b             2  \n                         aten::_safe_softmax         0.14%      21.753us         0.52%      80.798us      80.798us       4.00 Kb      -5.07 Kb             1  \n                               aten::softmax         0.02%       3.369us         0.09%      14.570us      14.570us       4.00 Kb           0 b             1  \n                              aten::_softmax         0.07%      11.201us         0.07%      11.201us      11.201us       4.00 Kb       4.00 Kb             1  \n                         aten::scalar_tensor         0.03%       4.853us         0.03%       4.853us       2.427us           8 b           8 b             2  \n                                    aten::eq         0.06%       8.623us         0.06%       8.623us       8.623us       1.00 Kb       1.00 Kb             1  \n                                   aten::all         0.09%      13.703us         0.10%      16.138us      16.138us          64 b          64 b             1  \n                                 aten::fill_         0.02%       2.435us         0.02%       2.435us       2.435us           0 b           0 b             1  \n                                 aten::where         0.09%      13.426us         0.10%      14.861us      14.861us       4.00 Kb       4.00 Kb             1  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 15.533ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e6001216b90>\nMulti Threaded SDPA - SmolAttention\nsetup: from __main__ import SmolAttention\n  442.75 us\n  1 measurement, 100 runs , 2 threads\n=============== Experiment 2: seq_len=8, hidden_dim=32 =========================\nNon-Quantized Model: \n=============== Memory Profile: ===========================\n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             model_inference        17.12%     185.324us       100.00%       1.083ms       1.083ms           0 b    -224.00 Kb             1  \n                                aten::linear         3.59%      38.861us        35.55%     384.869us     128.290us      96.00 Kb           0 b             3  \n                               aten::reshape         4.74%      51.279us         7.18%      77.747us      11.107us           0 b           0 b             7  \n                                  aten::view         2.90%      31.414us         2.90%      31.414us       3.490us           0 b           0 b             9  \n                                     aten::t         1.06%      11.477us         2.06%      22.299us       7.433us           0 b           0 b             3  \n                             aten::transpose         1.20%      12.966us         1.64%      17.794us       4.448us           0 b           0 b             4  \n                            aten::as_strided         0.91%       9.901us         0.91%       9.901us       0.900us           0 b           0 b            11  \n                                 aten::addmm        17.06%     184.645us        25.60%     277.171us      92.390us      96.00 Kb      96.00 Kb             3  \n                                aten::expand         2.40%      25.970us         2.87%      31.043us       4.435us           0 b           0 b             7  \n                                 aten::copy_         8.04%      87.074us         8.04%      87.074us      17.415us           0 b           0 b             5  \n                          aten::resolve_conj         0.14%       1.492us         0.14%       1.492us       0.149us           0 b           0 b            10  \n          aten::scaled_dot_product_attention         0.44%       4.727us        47.33%     512.380us     512.380us     128.00 Kb           0 b             1  \n    aten::_scaled_dot_product_attention_math         1.52%      16.417us        46.89%     507.653us     507.653us     128.00 Kb     -32.00 Kb             1  \n                                   aten::mul         9.28%     100.419us        11.15%     120.744us      60.372us      64.00 Kb      63.99 Kb             2  \n                                    aten::to         0.39%       4.272us         1.94%      20.973us       5.243us           8 b           0 b             4  \n                              aten::_to_copy         0.82%       8.931us         1.54%      16.701us       8.351us           8 b           0 b             2  \n                         aten::empty_strided         0.33%       3.626us         0.33%       3.626us       1.813us           8 b           8 b             2  \n                                aten::matmul         2.44%      26.423us        17.08%     184.944us      92.472us      64.00 Kb           0 b             2  \n                        aten::_reshape_alias         0.41%       4.428us         0.41%       4.428us       4.428us           0 b           0 b             1  \n                                   aten::bmm         8.21%      88.906us         8.27%      89.567us      44.784us      64.00 Kb      64.00 Kb             2  \n                          aten::_unsafe_view         0.56%       6.093us         0.56%       6.093us       3.046us           0 b           0 b             2  \n                         aten::_safe_softmax         1.82%      19.672us        16.44%     177.928us     177.928us      32.00 Kb     -40.26 Kb             1  \n                               aten::softmax         0.30%       3.272us         4.36%      47.198us      47.198us      32.00 Kb           0 b             1  \n                              aten::_softmax         4.06%      43.926us         4.06%      43.926us      43.926us      32.00 Kb      32.00 Kb             1  \n                         aten::scalar_tensor         0.38%       4.063us         0.38%       4.063us       2.031us           8 b           8 b             2  \n                                    aten::eq         2.18%      23.629us         2.18%      23.629us      23.629us       8.00 Kb       8.00 Kb             1  \n                                   aten::all         2.89%      31.236us         3.08%      33.293us      33.293us         256 b         256 b             1  \n                                 aten::fill_         0.19%       2.057us         0.19%       2.057us       2.057us           0 b           0 b             1  \n                                 aten::where         4.49%      48.560us         4.63%      50.073us      50.073us      32.00 Kb      32.00 Kb             1  \n                                 aten::empty         0.14%       1.513us         0.14%       1.513us       1.513us           0 b           0 b             1  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 1.083ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e5f21ccc3a0>\nMulti Threaded SDPA - SmolAttention\nsetup: from __main__ import SmolAttention\n  347.22 us\n  1 measurement, 100 runs , 2 threads\nQuantized Model: \n=============== Memory Profile: ===========================\n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             model_inference        37.11%     469.840us       100.00%       1.266ms       1.266ms           0 b    -224.00 Kb             1  \n                   quantized::linear_dynamic        21.87%     276.838us        24.02%     304.049us     101.350us      96.00 Kb     -96.00 Kb             3  \n                                 aten::empty         1.50%      19.052us         1.50%      19.052us       2.722us     192.00 Kb     192.00 Kb             7  \n                            aten::empty_like         0.76%       9.592us         1.05%      13.232us       4.411us      96.00 Kb           0 b             3  \n                                    aten::to         0.53%       6.740us         2.45%      31.074us       4.439us           8 b           0 b             7  \n          aten::scaled_dot_product_attention         0.46%       5.836us        38.75%     490.643us     490.643us     128.00 Kb           0 b             1  \n    aten::_scaled_dot_product_attention_math         1.57%      19.907us        38.29%     484.807us     484.807us     128.00 Kb     -32.00 Kb             1  \n                                   aten::mul         3.33%      42.167us         5.62%      71.159us      35.580us      64.00 Kb      63.99 Kb             2  \n                              aten::_to_copy         0.93%      11.774us         1.92%      24.334us      12.167us           8 b           0 b             2  \n                         aten::empty_strided         0.34%       4.249us         0.34%       4.249us       2.124us           8 b           8 b             2  \n                                 aten::copy_         0.66%       8.311us         0.66%       8.311us       4.155us           0 b           0 b             2  \n                             aten::transpose         0.71%       8.993us         0.90%      11.350us      11.350us           0 b           0 b             1  \n                            aten::as_strided         0.48%       6.130us         0.48%       6.130us       1.226us           0 b           0 b             5  \n                                aten::matmul         2.43%      30.779us        14.13%     178.923us      89.461us      64.00 Kb           0 b             2  \n                                aten::expand         1.80%      22.848us         2.10%      26.621us       6.655us           0 b           0 b             4  \n                               aten::reshape         1.01%      12.798us         2.20%      27.843us       6.961us           0 b           0 b             4  \n                                  aten::view         0.89%      11.222us         0.89%      11.222us       3.741us           0 b           0 b             3  \n                        aten::_reshape_alias         0.30%       3.823us         0.30%       3.823us       3.823us           0 b           0 b             1  \n                                   aten::bmm         6.05%      76.600us         6.12%      77.503us      38.752us      64.00 Kb      64.00 Kb             2  \n                          aten::resolve_conj         0.07%       0.903us         0.07%       0.903us       0.226us           0 b           0 b             4  \n                          aten::_unsafe_view         1.28%      16.177us         1.28%      16.177us       8.089us           0 b           0 b             2  \n                         aten::_safe_softmax         1.78%      22.546us        16.03%     202.896us     202.896us      32.00 Kb     -40.26 Kb             1  \n                               aten::softmax         0.26%       3.337us         4.22%      53.458us      53.458us      32.00 Kb           0 b             1  \n                              aten::_softmax         3.96%      50.121us         3.96%      50.121us      50.121us      32.00 Kb      32.00 Kb             1  \n                         aten::scalar_tensor         0.35%       4.408us         0.35%       4.408us       2.204us           8 b           8 b             2  \n                                    aten::eq         1.39%      17.645us         1.39%      17.645us      17.645us       8.00 Kb       8.00 Kb             1  \n                                   aten::all         2.78%      35.197us         2.95%      37.399us      37.399us         256 b         256 b             1  \n                                 aten::fill_         0.17%       2.202us         0.17%       2.202us       2.202us           0 b           0 b             1  \n                                 aten::where         5.21%      66.007us         5.33%      67.440us      67.440us      32.00 Kb      32.00 Kb             1  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 1.266ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e60012176a0>\nMulti Threaded SDPA - SmolAttention\nsetup: from __main__ import SmolAttention\n  552.11 us\n  1 measurement, 100 runs , 2 threads\n=============== Experiment 3: seq_len=10, hidden_dim=64 =========================\nNon-Quantized Model: \n=============== Memory Profile: ===========================\n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             model_inference         4.77%     221.957us       100.00%       4.650ms       4.650ms           0 b      -1.09 Mb             1  \n                                aten::linear         1.02%      47.270us        69.04%       3.210ms       1.070ms     480.00 Kb           0 b             3  \n                               aten::reshape         0.41%      19.119us         1.11%      51.744us       7.392us           0 b           0 b             7  \n                                  aten::view         1.01%      47.174us         1.01%      47.174us       5.242us           0 b           0 b             9  \n                                     aten::t         0.32%      14.900us         0.64%      29.612us       9.871us           0 b           0 b             3  \n                             aten::transpose         0.41%      18.843us         0.54%      25.171us       6.293us           0 b           0 b             4  \n                            aten::as_strided         0.27%      12.684us         0.27%      12.684us       1.153us           0 b           0 b            11  \n                                 aten::addmm        63.36%       2.946ms        66.48%       3.091ms       1.030ms     480.00 Kb     480.00 Kb             3  \n                                aten::expand         0.74%      34.436us         0.88%      40.792us       5.827us           0 b           0 b             7  \n                                 aten::copy_         2.99%     138.937us         2.99%     138.937us      27.787us           0 b           0 b             5  \n                          aten::resolve_conj         0.05%       2.336us         0.05%       2.336us       0.234us           0 b           0 b            10  \n          aten::scaled_dot_product_attention         0.12%       5.490us        26.19%       1.218ms       1.218ms     640.00 Kb           0 b             1  \n    aten::_scaled_dot_product_attention_math         0.50%      23.315us        26.07%       1.213ms       1.213ms     640.00 Kb    -160.00 Kb             1  \n                                   aten::mul         4.92%     228.853us         5.51%     256.453us     128.226us     320.00 Kb     319.99 Kb             2  \n                                    aten::to         0.12%       5.429us         0.62%      28.707us       7.177us           8 b           0 b             4  \n                              aten::_to_copy         0.26%      11.982us         0.50%      23.278us      11.639us           8 b           0 b             2  \n                         aten::empty_strided         0.10%       4.802us         0.10%       4.802us       2.401us           8 b           8 b             2  \n                                aten::matmul         0.72%      33.392us         8.83%     410.788us     205.394us     320.00 Kb           0 b             2  \n                        aten::_reshape_alias         0.09%       4.183us         0.09%       4.183us       4.183us           0 b           0 b             1  \n                                   aten::bmm         6.68%     310.804us         6.70%     311.565us     155.783us     320.00 Kb     320.00 Kb             2  \n                          aten::_unsafe_view         0.17%       7.710us         0.17%       7.710us       3.855us           0 b           0 b             2  \n                         aten::_safe_softmax         0.62%      28.866us        10.98%     510.434us     510.434us     160.00 Kb    -200.63 Kb             1  \n                               aten::softmax         0.07%       3.045us         3.30%     153.416us     153.416us     160.00 Kb           0 b             1  \n                              aten::_softmax         3.23%     150.371us         3.23%     150.371us     150.371us     160.00 Kb     160.00 Kb             1  \n                         aten::scalar_tensor         0.12%       5.756us         0.12%       5.756us       2.878us           8 b           8 b             2  \n                                    aten::eq         1.29%      60.083us         1.29%      60.083us      60.083us      40.00 Kb      40.00 Kb             1  \n                                   aten::all         1.69%      78.448us         1.74%      80.880us      80.880us         640 b         640 b             1  \n                                 aten::fill_         0.05%       2.432us         0.05%       2.432us       2.432us           0 b           0 b             1  \n                                 aten::where         3.87%     179.992us         3.90%     181.433us     181.433us     160.00 Kb     160.00 Kb             1  \n                                 aten::empty         0.03%       1.441us         0.03%       1.441us       1.441us           0 b           0 b             1  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 4.650ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e5f21b3afb0>\nMulti Threaded SDPA - SmolAttention\nsetup: from __main__ import SmolAttention\n  664.73 us\n  1 measurement, 100 runs , 2 threads\nQuantized Model: \n=============== Memory Profile: ===========================\n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             model_inference        27.22%     399.837us       100.00%       1.469ms       1.469ms           0 b      -1.09 Mb             1  \n                   quantized::linear_dynamic        23.73%     348.507us        27.72%     407.207us     135.736us     480.00 Kb    -480.00 Kb             3  \n                                 aten::empty         1.53%      22.443us         1.53%      22.443us       3.206us     960.00 Kb     960.00 Kb             7  \n                            aten::empty_like         2.56%      37.580us         2.91%      42.709us      14.236us     480.00 Kb           0 b             3  \n                                    aten::to         0.39%       5.693us         1.88%      27.645us       3.949us           8 b           0 b             7  \n          aten::scaled_dot_product_attention         0.36%       5.237us        44.97%     660.491us     660.491us     640.00 Kb           0 b             1  \n    aten::_scaled_dot_product_attention_math         1.13%      16.529us        44.61%     655.254us     655.254us     640.00 Kb    -160.00 Kb             1  \n                                   aten::mul         3.42%      50.212us         5.16%      75.838us      37.919us     320.00 Kb     319.99 Kb             2  \n                              aten::_to_copy         0.72%      10.614us         1.49%      21.952us      10.976us           8 b           0 b             2  \n                         aten::empty_strided         0.26%       3.868us         0.26%       3.868us       1.934us           8 b           8 b             2  \n                                 aten::copy_         0.51%       7.470us         0.51%       7.470us       3.735us           0 b           0 b             2  \n                             aten::transpose         0.55%       8.135us         0.71%      10.395us      10.395us           0 b           0 b             1  \n                            aten::as_strided         0.37%       5.505us         0.37%       5.505us       1.101us           0 b           0 b             5  \n                                aten::matmul         1.85%      27.219us        19.91%     292.454us     146.227us     320.00 Kb           0 b             2  \n                                aten::expand         1.37%      20.148us         1.59%      23.393us       5.848us           0 b           0 b             4  \n                               aten::reshape         0.76%      11.118us         1.67%      24.578us       6.145us           0 b           0 b             4  \n                                  aten::view         0.69%      10.110us         0.69%      10.110us       3.370us           0 b           0 b             3  \n                        aten::_reshape_alias         0.23%       3.350us         0.23%       3.350us       3.350us           0 b           0 b             1  \n                                   aten::bmm        14.28%     209.715us        14.34%     210.628us     105.314us     320.00 Kb     320.00 Kb             2  \n                          aten::resolve_conj         0.06%       0.913us         0.06%       0.913us       0.228us           0 b           0 b             4  \n                          aten::_unsafe_view         0.45%       6.636us         0.45%       6.636us       3.318us           0 b           0 b             2  \n                         aten::_safe_softmax         2.69%      39.511us        17.66%     259.333us     259.333us     160.00 Kb    -200.63 Kb             1  \n                               aten::softmax         0.19%       2.806us         3.84%      56.361us      56.361us     160.00 Kb           0 b             1  \n                              aten::_softmax         3.65%      53.555us         3.65%      53.555us      53.555us     160.00 Kb     160.00 Kb             1  \n                         aten::scalar_tensor         0.32%       4.703us         0.32%       4.703us       2.351us           8 b           8 b             2  \n                                    aten::eq         2.19%      32.165us         2.19%      32.165us      32.165us      40.00 Kb      40.00 Kb             1  \n                                   aten::all         5.56%      81.602us         5.69%      83.582us      83.582us         640 b         640 b             1  \n                                 aten::fill_         0.13%       1.980us         0.13%       1.980us       1.980us           0 b           0 b             1  \n                                 aten::where         2.84%      41.688us         2.93%      43.011us      43.011us     160.00 Kb     160.00 Kb             1  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 1.469ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e5f3a5c8070>\nMulti Threaded SDPA - SmolAttention\nsetup: from __main__ import SmolAttention\n  901.69 us\n  1 measurement, 100 runs , 2 threads\n=============== Experiment 4: seq_len=12, hidden_dim=128 =========================\nNon-Quantized Model: \n=============== Memory Profile: ===========================\n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             model_inference         3.70%     205.251us       100.00%       5.541ms       5.541ms           0 b      -5.25 Mb             1  \n                                aten::linear         0.82%      45.333us        30.40%       1.684ms     561.466us       2.25 Mb           0 b             3  \n                               aten::reshape         0.30%      16.384us         0.81%      45.063us       6.438us           0 b           0 b             7  \n                                  aten::view         0.82%      45.185us         0.82%      45.185us       5.021us           0 b           0 b             9  \n                                     aten::t         0.27%      14.870us         0.53%      29.224us       9.741us           0 b           0 b             3  \n                             aten::transpose         0.33%      18.128us         0.45%      25.051us       6.263us           0 b           0 b             4  \n                            aten::as_strided         0.24%      13.195us         0.24%      13.195us       1.200us           0 b           0 b            11  \n                                 aten::addmm        20.02%       1.109ms        28.29%       1.567ms     522.436us       2.25 Mb       2.25 Mb             3  \n                                aten::expand         0.50%      27.924us         0.62%      34.196us       4.885us           0 b           0 b             7  \n                                 aten::copy_         8.18%     453.296us         8.18%     453.296us      90.659us           0 b           0 b             5  \n                          aten::resolve_conj         0.04%       2.032us         0.04%       2.032us       0.203us           0 b           0 b            10  \n          aten::scaled_dot_product_attention         0.10%       5.802us        65.90%       3.651ms       3.651ms       3.00 Mb           0 b             1  \n    aten::_scaled_dot_product_attention_math         0.41%      22.813us        65.79%       3.646ms       3.646ms       3.00 Mb    -768.00 Kb             1  \n                                   aten::mul        14.98%     830.183us        15.50%     858.729us     429.364us       1.50 Mb       1.50 Mb             2  \n                                    aten::to         0.10%       5.645us         0.53%      29.528us       7.382us           8 b           0 b             4  \n                              aten::_to_copy         0.22%      12.009us         0.43%      23.883us      11.941us           8 b           0 b             2  \n                         aten::empty_strided         0.09%       4.912us         0.09%       4.912us       2.456us           8 b           8 b             2  \n                                aten::matmul         0.59%      32.588us        24.39%       1.351ms     675.675us       1.50 Mb           0 b             2  \n                        aten::_reshape_alias         0.06%       3.068us         0.06%       3.068us       3.068us           0 b           0 b             1  \n                                   aten::bmm        22.78%       1.262ms        22.79%       1.263ms     631.543us       1.50 Mb       1.50 Mb             2  \n                          aten::_unsafe_view         0.18%      10.005us         0.18%      10.005us       5.003us           0 b           0 b             2  \n                         aten::_safe_softmax         0.47%      26.087us        25.28%       1.401ms       1.401ms     768.00 Kb    -961.51 Kb             1  \n                               aten::softmax         0.08%       4.343us        10.66%     590.532us     590.532us     768.00 Kb           0 b             1  \n                              aten::_softmax        10.58%     586.189us        10.58%     586.189us     586.189us     768.00 Kb     768.00 Kb             1  \n                         aten::scalar_tensor         0.11%       6.071us         0.11%       6.071us       3.035us           8 b           8 b             2  \n                                    aten::eq         2.17%     120.248us         2.17%     120.248us     120.248us     192.00 Kb     192.00 Kb             1  \n                                   aten::all         2.07%     114.696us         2.12%     117.481us     117.481us       1.50 Kb       1.50 Kb             1  \n                                 aten::fill_         0.05%       2.785us         0.05%       2.785us       2.785us           0 b           0 b             1  \n                                 aten::where         9.73%     539.192us         9.76%     540.631us     540.631us     768.00 Kb     768.00 Kb             1  \n                                 aten::empty         0.03%       1.439us         0.03%       1.439us       1.439us           0 b           0 b             1  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 5.541ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e5f21bae9e0>\nMulti Threaded SDPA - SmolAttention\nsetup: from __main__ import SmolAttention\n  2.27 ms\n  1 measurement, 100 runs , 2 threads\nQuantized Model: \n=============== Memory Profile: ===========================\n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             model_inference        14.57%     615.196us       100.00%       4.222ms       4.222ms           0 b      -5.25 Mb             1  \n                   quantized::linear_dynamic        26.31%       1.111ms        27.22%       1.149ms     383.078us       2.25 Mb      -2.25 Mb             3  \n                                 aten::empty         0.65%      27.317us         0.65%      27.317us       3.902us       4.50 Mb       4.50 Mb             7  \n                            aten::empty_like         0.29%      12.216us         0.39%      16.500us       5.500us       2.25 Mb           0 b             3  \n                                    aten::to         0.20%       8.300us         0.85%      35.805us       5.115us           8 b           0 b             7  \n          aten::scaled_dot_product_attention         0.17%       7.026us        58.17%       2.456ms       2.456ms       3.00 Mb           0 b             1  \n    aten::_scaled_dot_product_attention_math         0.57%      24.119us        58.00%       2.449ms       2.449ms       3.00 Mb    -768.00 Kb             1  \n                                   aten::mul         2.83%     119.603us         3.61%     152.565us      76.283us       1.50 Mb       1.50 Mb             2  \n                              aten::_to_copy         0.32%      13.493us         0.65%      27.505us      13.753us           8 b           0 b             2  \n                         aten::empty_strided         0.12%       5.085us         0.12%       5.085us       2.543us           8 b           8 b             2  \n                                 aten::copy_         0.21%       8.927us         0.21%       8.927us       4.463us           0 b           0 b             2  \n                             aten::transpose         0.27%      11.318us         0.34%      14.174us      14.174us           0 b           0 b             1  \n                            aten::as_strided         0.17%       7.299us         0.17%       7.299us       1.460us           0 b           0 b             5  \n                                aten::matmul         0.92%      38.978us        27.96%       1.180ms     590.186us       1.50 Mb           0 b             2  \n                                aten::expand         0.52%      22.098us         0.63%      26.541us       6.635us           0 b           0 b             4  \n                               aten::reshape         0.32%      13.428us         0.66%      27.890us       6.972us           0 b           0 b             4  \n                                  aten::view         0.25%      10.423us         0.25%      10.423us       3.474us           0 b           0 b             3  \n                        aten::_reshape_alias         0.10%       4.039us         0.10%       4.039us       4.039us           0 b           0 b             1  \n                                   aten::bmm        25.46%       1.075ms        25.48%       1.076ms     537.962us       1.50 Mb       1.50 Mb             2  \n                          aten::resolve_conj         0.02%       0.888us         0.02%       0.888us       0.222us           0 b           0 b             4  \n                          aten::_unsafe_view         0.26%      11.037us         0.26%      11.037us       5.519us           0 b           0 b             2  \n                         aten::_safe_softmax         0.65%      27.283us        25.49%       1.076ms       1.076ms     768.00 Kb    -961.51 Kb             1  \n                               aten::softmax         0.09%       3.790us         5.74%     242.519us     242.519us     768.00 Kb           0 b             1  \n                              aten::_softmax         5.65%     238.729us         5.65%     238.729us     238.729us     768.00 Kb     768.00 Kb             1  \n                         aten::scalar_tensor         0.13%       5.670us         0.13%       5.670us       2.835us           8 b           8 b             2  \n                                    aten::eq         3.13%     131.936us         3.13%     131.936us     131.936us     192.00 Kb     192.00 Kb             1  \n                                   aten::all         2.66%     112.510us         2.73%     115.140us     115.140us       1.50 Kb       1.50 Kb             1  \n                                 aten::fill_         0.06%       2.630us         0.06%       2.630us       2.630us           0 b           0 b             1  \n                                 aten::where        13.09%     552.573us        13.12%     553.798us     553.798us     768.00 Kb     768.00 Kb             1  \n--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 4.222ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e60001fb490>\nMulti Threaded SDPA - SmolAttention\nsetup: from __main__ import SmolAttention\n  2.90 ms\n  1 measurement, 100 runs , 2 threads\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## FlexAttention Tests\nNow, we move on to the difficult stuff. If we are able to do this, things will be the easiest.\n\n**Update:** This was easier than I though it would be, I was able to do it one shot. Now, we move on it difficult stuff. Just run experiments and see how to approach this difficult problem.","metadata":{}},{"cell_type":"code","source":"## Actual FlexAttention Call\n\nimport torch\nfrom torch.nn.attention.flex_attention import flex_attention\n\n\n@torch.compile \ndef no_op(score, b, h, q_idx, kv_idx):\n    return score\n\nprint(\"*********************** FlexAttention SDPA Tests *************************\")\n\nbatch_sizes = [8, 16, 32]\nseq_lengths = [8, 16, 32]\nhidden_dims = [64, 128, 256]\n\n\nfor i, (bs, sl, hd) in enumerate(zip(batch_sizes, seq_lengths, hidden_dims)):\n    print(f\"=============== Experiment {i+1}: batch_size={bs} seq_len={sl}, hidden_dim={hd} =========================\")\n    q = torch.randn(size=(bs, sl, hd, hd), requires_grad=True, device=\"cuda\")\n    k = torch.randn(size=(bs, sl, hd, hd), requires_grad=True, device=\"cuda\")\n    v = torch.randn(size=(bs, sl, hd, hd), requires_grad=True, device=\"cuda\")\n\n    \n    pa = PerformanceAnalysis(flex_attention, (q,k,v), no_op)    # extra score_mod is added\n    print(\"=============== Memory Profile: ===========================\")\n    print(pa.profile())\n    \n    print(\"=============== Benchmark Report: =========================\")\n    print(pa.benchmark())\n    \n    \n    del q,k,v,pa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T16:21:57.491788Z","iopub.execute_input":"2025-01-27T16:21:57.492126Z","iopub.status.idle":"2025-01-27T16:22:08.227778Z","shell.execute_reply.started":"2025-01-27T16:21:57.492100Z","shell.execute_reply":"2025-01-27T16:22:08.227047Z"}},"outputs":[{"name":"stdout","text":"*********************** FlexAttention SDPA Tests *************************\n=============== Experiment 1: batch_size=8 seq_len=8, hidden_dim=64 =========================\n=============== Memory Profile: ===========================\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                        model_inference        13.21%      68.260ms       100.00%     516.692ms     516.692ms           0 b           0 b             1  \n                                             aten::ones         0.00%      25.530us         0.16%     815.273us     815.273us           0 b           0 b             1  \n                                            aten::empty         0.21%       1.075ms         0.36%       1.872ms      39.002us          40 b          40 b            48  \n                                            aten::fill_         0.02%     110.303us         0.06%     303.833us      37.979us           0 b           0 b             8  \n                                       cudaLaunchKernel        47.32%     244.498ms        47.32%     244.498ms       5.433ms           0 b           0 b            45  \n                                            aten::zeros         0.01%      61.918us         0.02%     111.763us      18.627us          20 b           0 b             6  \n                                            aten::zero_         0.00%      15.636us         0.01%      63.687us       9.098us           0 b           0 b             7  \n                                        aten::new_zeros         0.01%      61.265us         0.02%     118.590us     118.590us           0 b           0 b             1  \n                                        aten::new_empty         0.92%       4.777ms         1.02%       5.273ms     210.903us           0 b           0 b            25  \n                                           aten::arange         0.59%       3.050ms         3.79%      19.570ms       1.087ms          32 b           0 b            18  \n                                          aten::resize_         0.02%     108.108us         0.02%     108.108us       7.207us          16 b          16 b            15  \n                                        aten::unsqueeze         0.02%      84.320us         0.02%     101.415us      11.268us           0 b           0 b             9  \n                                       aten::as_strided         0.01%      70.008us         0.01%      70.008us       2.000us           0 b           0 b            35  \n                                               aten::lt         0.17%     858.427us         2.14%      11.066ms      11.066ms           0 b           0 b             1  \n                                            aten::where         0.03%     144.233us         0.10%     509.794us      84.966us           0 b           0 b             6  \n                                    aten::scalar_tensor         0.01%      41.976us         0.03%     156.780us      39.195us           0 b           0 b             4  \n                                               aten::to         0.01%      61.591us         3.69%      19.057ms       1.270ms           0 b           0 b            15  \n                                       aten::lift_fresh         0.00%      13.755us         0.00%      13.755us       6.878us           0 b           0 b             2  \n                                       aten::index_put_         0.23%       1.203ms        27.90%     144.174ms      48.058ms           0 b         -16 b             3  \n                                         aten::_to_copy         0.02%      89.817us         3.68%      18.996ms       2.111ms           0 b           0 b             9  \n                                    aten::empty_strided         0.64%       3.307ms         0.65%       3.338ms      75.873us           8 b           8 b            44  \n                                            aten::copy_         0.59%       3.041ms         4.00%      20.649ms       1.291ms           0 b           0 b            16  \n                                        cudaMemcpyAsync         0.35%       1.796ms         0.35%       1.796ms     179.572us           0 b           0 b            10  \n                                  cudaStreamSynchronize         0.01%      53.150us         0.01%      53.150us      13.288us           0 b           0 b             4  \n                                           aten::expand         0.17%     876.594us         0.18%     934.100us      49.163us           0 b           0 b            19  \n                                             aten::view         0.01%      49.101us         0.01%      49.101us       3.507us           0 b           0 b            14  \n                                 aten::_index_put_impl_         0.35%       1.810ms         8.95%      46.238ms      46.238ms           0 b           0 b             1  \n                                          aten::reshape         0.00%      18.973us         0.01%      32.283us       4.035us           0 b           0 b             8  \n                                            aten::slice         0.27%       1.409ms         0.27%       1.415ms     707.435us           0 b           0 b             2  \n                                        aten::transpose         0.00%      13.304us         0.00%      16.171us       8.086us           0 b           0 b             2  \n                                              aten::sum         0.60%       3.091ms         4.27%      22.046ms      11.023ms           0 b           0 b             2  \n                                          aten::argsort         0.00%      14.477us         0.55%       2.864ms       2.864ms           0 b           0 b             1  \n                                             aten::sort         0.20%       1.031ms         0.55%       2.850ms       2.850ms           0 b           0 b             1  \n                               TorchDynamo Cache Lookup         0.00%       1.701us         0.00%       1.701us       0.425us           0 b           0 b             4  \n                  _compile.compile_inner (dynamo_timed)        27.61%     142.654ms        29.46%     152.233ms     152.233ms           0 b         -16 b             1  \n                                  cudaStreamIsCapturing         0.00%      16.696us         0.00%      16.696us       2.783us           0 b           0 b             6  \n                                           aten::detach         0.02%     116.348us         0.03%     171.769us       4.090us           0 b           0 b            42  \n                                                 detach         0.01%      55.421us         0.01%      55.421us       3.260us           0 b           0 b            17  \n                                         aten::new_ones         0.64%       3.281ms         0.69%       3.567ms       1.189ms           0 b           0 b             3  \n                                             aten::full         0.01%      59.050us         0.01%      68.265us      68.265us           0 b           0 b             1  \n                                       aten::empty_like         0.15%     783.484us         0.33%       1.681ms     336.209us          16 b           0 b             5  \n          OutputGraph.call_user_compiler (dynamo_timed)         0.03%     169.166us         0.03%     169.166us     169.166us           0 b           0 b             1  \n                                            aten::clone         0.00%      24.059us         0.19%     987.894us     329.298us          16 b           0 b             3  \n                                  Torch-Compiled Region         2.55%      13.190ms        38.70%     199.956ms     199.956ms           0 b         -28 b             1  \nautograd::engine::evaluate_function: torch::autograd...         0.00%       9.968us         0.00%       9.968us       9.968us           0 b           0 b             1  \n                                FlexAttentionAutogradOp         0.71%       3.679ms        35.83%     185.135ms     185.135ms           0 b          -4 b             1  \n                                aten::repeat_interleave         0.19%     972.925us         0.38%       1.965ms     982.551us           0 b           0 b             2  \n                                             cudaMalloc         0.23%       1.178ms         0.23%       1.178ms     392.540us           0 b           0 b             3  \n                                          aten::flatten         0.00%       8.325us         0.00%      15.684us       7.842us           0 b           0 b             2  \n                                           aten::matmul         0.01%      42.752us         0.10%     534.055us     267.027us           0 b           0 b             2  \n                                   aten::_reshape_alias         0.00%       2.577us         0.00%       2.577us       2.577us           0 b           0 b             1  \n                                              aten::bmm         0.04%     227.170us         0.09%     444.867us     222.434us           0 b           0 b             2  \n          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.01%      29.633us         0.01%      29.633us       2.469us           0 b           0 b            12  \n                                     aten::_unsafe_view         0.00%       4.423us         0.00%       4.423us       2.211us           0 b           0 b             2  \n                                              aten::mul         0.01%      30.918us         0.01%      41.131us      41.131us           0 b           0 b             1  \n                                          aten::detach_         0.00%       0.595us         0.00%       0.595us       0.595us           0 b           0 b             1  \n                                        aten::logsumexp         0.04%     198.556us        61.68%     318.678ms     159.339ms           0 b           0 b             2  \n                                             aten::real         0.00%       2.276us         0.00%       2.276us       2.276us           0 b           0 b             1  \n                                             aten::amax         0.16%     805.128us         3.86%      19.950ms      19.950ms           0 b           0 b             1  \n                                          aten::squeeze         0.00%       6.651us         0.00%       9.593us       9.593us           0 b           0 b             1  \n                                              aten::abs         0.02%     101.236us         3.69%      19.087ms       9.543ms           0 b           0 b             2  \n                                               aten::eq         0.02%     103.804us         0.03%     154.905us      51.635us           0 b           0 b             3  \n                                     aten::masked_fill_         0.51%       2.661ms        16.21%      83.732ms      83.732ms           0 b           0 b             1  \n                                              aten::sub         0.01%      65.906us         0.06%     314.569us     314.569us           0 b           0 b             1  \n                                             aten::exp_         0.39%       2.017ms         5.46%      28.221ms      28.221ms           0 b           0 b             1  \n                                             aten::log_         0.27%       1.420ms         2.88%      14.876ms      14.876ms           0 b           0 b             1  \n                                             aten::add_         0.00%      21.470us         0.01%      51.316us      51.316us           0 b           0 b             1  \n                                              aten::all         0.01%      49.566us         0.01%      75.566us      37.783us           0 b           0 b             2  \n                                    aten::_safe_softmax         0.00%      25.731us         0.04%     210.514us     210.514us           0 b           0 b             1  \n                                          aten::softmax         0.00%       4.944us         0.01%      51.316us      51.316us           0 b           0 b             1  \n                                         aten::_softmax         0.01%      32.601us         0.01%      46.372us      46.372us           0 b           0 b             1  \n                                              aten::div         0.29%       1.520ms         3.34%      17.257ms      17.257ms           0 b           0 b             1  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 516.692ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e5f6a068dc0>\nMulti Threaded SDPA - flex_attention\nsetup: from __main__ import no_op\n  8.45 ms\n  1 measurement, 100 runs , 2 threads\n=============== Experiment 2: batch_size=16 seq_len=16, hidden_dim=128 =========================\n=============== Memory Profile: ===========================\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                        model_inference         0.44%       1.520ms       100.00%     348.271ms     348.271ms           0 b           0 b             1  \n                                             aten::ones         0.01%      21.071us         0.95%       3.319ms       3.319ms           0 b           0 b             1  \n                                            aten::empty         7.13%      24.841ms         9.17%      31.921ms     560.014us          40 b          40 b            57  \n                                            aten::fill_         0.02%      71.865us         0.04%     140.381us      17.548us           0 b           0 b             8  \n                                       cudaLaunchKernel         0.09%     323.536us         0.09%     323.536us       7.190us           0 b           0 b            45  \n                                            aten::zeros         0.01%      26.241us         0.02%      64.099us      10.683us          20 b           0 b             6  \n                                            aten::zero_         0.01%      30.639us         0.02%      63.188us       9.027us           0 b           0 b             7  \n                                        aten::new_zeros         0.01%      33.037us         0.03%      91.585us      91.585us           0 b           0 b             1  \n                                        aten::new_empty         0.62%       2.153ms         2.66%       9.259ms     420.844us           0 b           0 b            22  \n                                           aten::arange         0.04%     126.515us         0.11%     390.819us      21.712us          32 b           0 b            18  \n                                          aten::resize_         0.02%      76.729us         0.12%     408.645us      27.243us          16 b          16 b            15  \n                                        aten::unsqueeze         0.02%      62.756us         0.02%      74.868us       8.319us           0 b           0 b             9  \n                                       aten::as_strided         0.01%      38.605us         0.01%      38.605us       1.103us           0 b           0 b            35  \n                                               aten::lt         0.01%      41.330us         0.01%      47.210us      47.210us           0 b           0 b             1  \n                                            aten::where         0.03%      96.281us         0.18%     630.185us     105.031us           0 b           0 b             6  \n                                    aten::scalar_tensor         0.01%      19.095us         0.03%      97.074us      24.268us           0 b           0 b             4  \n                                               aten::to         0.01%      21.192us         0.08%     271.863us      18.124us           0 b           0 b            15  \n                                       aten::lift_fresh         0.00%       5.473us         0.00%       5.473us       2.737us           0 b           0 b             2  \n                                       aten::index_put_         0.01%      49.076us         0.20%     684.146us     228.049us           0 b         -16 b             3  \n                                         aten::_to_copy         0.01%      39.349us         0.07%     250.671us      27.852us           0 b           0 b             9  \n                                    aten::empty_strided         6.67%      23.228ms         6.68%      23.248ms     553.525us           8 b           8 b            42  \n                                            aten::copy_         0.04%     128.458us         0.09%     303.399us      18.962us           0 b           0 b            16  \n                                        cudaMemcpyAsync         0.03%     118.761us         0.03%     118.761us      11.876us           0 b           0 b            10  \n                                  cudaStreamSynchronize         0.01%      23.500us         0.01%      23.500us       5.875us           0 b           0 b             4  \n                                           aten::expand         0.04%     126.395us         0.05%     180.601us       9.505us           0 b           0 b            19  \n                                             aten::view         0.01%      37.392us         0.01%      37.392us       2.671us           0 b           0 b            14  \n                                 aten::_index_put_impl_         0.01%      32.507us         0.04%     143.459us     143.459us           0 b           0 b             1  \n                                          aten::reshape         0.00%      15.891us         0.01%      26.317us       3.290us           0 b           0 b             8  \n                                            aten::slice         0.01%      28.080us         0.01%      29.472us      14.736us           0 b           0 b             2  \n                                        aten::transpose         0.00%      10.616us         0.00%      12.881us       6.440us           0 b           0 b             2  \n                                              aten::sum         0.01%      41.903us         0.03%      91.310us      45.655us           0 b           0 b             2  \n                                          aten::argsort         0.00%       3.085us         0.02%      86.820us      86.820us           0 b           0 b             1  \n                                             aten::sort         0.01%      22.092us         0.02%      83.735us      83.735us           0 b           0 b             1  \n                               TorchDynamo Cache Lookup         0.00%       8.888us         0.00%       8.888us       8.888us           0 b           0 b             1  \n                  _compile.compile_inner (dynamo_timed)        82.25%     286.442ms        95.68%     333.241ms     333.241ms           0 b         -16 b             1  \n                                  cudaStreamIsCapturing         0.00%      10.372us         0.00%      10.372us       2.593us           0 b           0 b             4  \n                                           aten::detach         0.04%     131.923us         0.06%     206.655us       5.299us           0 b           0 b            39  \n                                                 detach         0.02%      74.732us         0.02%      74.732us       4.396us           0 b           0 b            17  \n                                         aten::new_ones         0.07%     242.530us         0.09%     299.505us     149.752us           0 b           0 b             2  \n                                       aten::empty_like         0.07%     255.268us         5.23%      18.201ms       4.550ms          16 b           0 b             4  \n          OutputGraph.call_user_compiler (dynamo_timed)         0.05%     174.679us         0.05%     174.679us     174.679us           0 b           0 b             1  \n                                            aten::clone         0.01%      18.220us         0.05%     175.722us      58.574us          16 b           0 b             3  \n                                  Torch-Compiled Region         1.55%       5.413ms         2.66%       9.252ms       9.252ms           0 b         -28 b             1  \nautograd::engine::evaluate_function: torch::autograd...         0.00%       2.416us         0.00%       2.416us       2.416us           0 b           0 b             1  \n                                FlexAttentionAutogradOp         0.34%       1.182ms         0.88%       3.050ms       3.050ms           0 b          -4 b             1  \n                                aten::repeat_interleave         0.01%      21.047us         0.06%     199.018us      99.509us           0 b           0 b             2  \n                                          aten::flatten         0.00%       6.844us         0.00%      12.710us       6.355us           0 b           0 b             2  \n                                           aten::matmul         0.01%      34.797us         0.07%     260.072us     130.036us           0 b           0 b             2  \n                                   aten::_reshape_alias         0.00%       2.906us         0.00%       2.906us       2.906us           0 b           0 b             1  \n                                              aten::bmm         0.04%     143.140us         0.05%     187.043us      93.522us           0 b           0 b             2  \n          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.01%      19.660us         0.01%      19.660us       1.638us           0 b           0 b            12  \n                                     aten::_unsafe_view         0.00%       2.870us         0.00%       2.870us       1.435us           0 b           0 b             2  \n                                              aten::mul         0.01%      28.461us         0.01%      36.573us      36.573us           0 b           0 b             1  \n                                          aten::detach_         0.00%       0.442us         0.00%       0.442us       0.442us           0 b           0 b             1  \n                                        aten::logsumexp         0.02%      58.069us         0.15%     533.276us     266.638us           0 b           0 b             2  \n                                             aten::real         0.00%       0.570us         0.00%       0.570us       0.570us           0 b           0 b             1  \n                                             aten::amax         0.01%      23.891us         0.01%      33.794us      33.794us           0 b           0 b             1  \n                                          aten::squeeze         0.00%       3.788us         0.00%       5.291us       5.291us           0 b           0 b             1  \n                                              aten::abs         0.00%      14.741us         0.01%      51.769us      25.884us           0 b           0 b             2  \n                                               aten::eq         0.01%      50.504us         0.02%      67.049us      22.350us           0 b           0 b             3  \n                                     aten::masked_fill_         0.00%      11.919us         0.01%      17.735us      17.735us           0 b           0 b             1  \n                                              aten::sub         0.00%      16.079us         0.01%      23.317us      23.317us           0 b           0 b             1  \n                                             aten::exp_         0.00%       9.369us         0.00%      15.051us      15.051us           0 b           0 b             1  \n                                             aten::log_         0.00%       8.899us         0.00%      14.852us      14.852us           0 b           0 b             1  \n                                             aten::add_         0.00%       7.536us         0.00%      12.564us      12.564us           0 b           0 b             1  \n                                              aten::all         0.01%      31.895us         0.01%      47.213us      23.607us           0 b           0 b             2  \n                                    aten::_safe_softmax         0.01%      34.313us         0.15%     524.681us     524.681us           0 b           0 b             1  \n                                          aten::softmax         0.00%       3.754us         0.01%      24.935us      24.935us           0 b           0 b             1  \n                                         aten::_softmax         0.00%      14.262us         0.01%      21.181us      21.181us           0 b           0 b             1  \n                                             cudaMalloc         0.09%     330.036us         0.09%     330.036us     330.036us           0 b           0 b             1  \n                                              aten::div         0.01%      22.309us         0.01%      30.306us      30.306us           0 b           0 b             1  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 348.271ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e5f3aa599c0>\nMulti Threaded SDPA - flex_attention\nsetup: from __main__ import no_op\n  9.60 ms\n  1 measurement, 100 runs , 2 threads\n=============== Experiment 3: batch_size=32 seq_len=32, hidden_dim=256 =========================\n=============== Memory Profile: ===========================\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                        model_inference         0.85%       1.535ms       100.00%     180.934ms     180.934ms           0 b           0 b             1  \n                                             aten::ones         0.02%      40.962us         0.98%       1.775ms       1.775ms           0 b           0 b             1  \n                                            aten::empty         3.91%       7.074ms         5.69%      10.294ms     187.167us          40 b          40 b            55  \n                                            aten::fill_         0.06%     100.771us         0.10%     181.084us      22.635us           0 b           0 b             8  \n                                       cudaLaunchKernel         0.19%     351.401us         0.19%     351.401us       7.809us           0 b           0 b            45  \n                                            aten::zeros         0.01%      20.374us         0.03%      57.820us       9.637us          20 b           0 b             6  \n                                            aten::zero_         0.01%      11.643us         0.04%      68.142us       9.735us           0 b           0 b             7  \n                                        aten::new_zeros         0.02%      33.279us         0.05%      95.238us      95.238us           0 b           0 b             1  \n                                        aten::new_empty         1.09%       1.978ms         2.87%       5.194ms     236.100us           0 b           0 b            22  \n                                           aten::arange         0.09%     156.967us         0.25%     456.284us      25.349us          32 b           8 b            18  \n                                          aten::resize_         0.06%     100.377us         0.16%     294.376us      19.625us           8 b           8 b            15  \n                                        aten::unsqueeze         0.04%      67.287us         0.04%      80.396us       8.933us           0 b           0 b             9  \n                                       aten::as_strided         0.02%      42.241us         0.02%      42.241us       1.207us           0 b           0 b            35  \n                                               aten::lt         0.02%      30.214us         0.02%      36.951us      36.951us           0 b           0 b             1  \n                                            aten::where         0.06%     114.801us         0.32%     583.308us      97.218us           0 b           0 b             6  \n                                    aten::scalar_tensor         0.01%      22.975us         0.05%      97.571us      24.393us           0 b           0 b             4  \n                                               aten::to         0.01%      26.072us        11.16%      20.184ms       1.346ms           0 b           0 b            15  \n                                       aten::lift_fresh         0.00%       6.147us         0.00%       6.147us       3.074us           0 b           0 b             2  \n                                       aten::index_put_         0.03%      59.716us         1.52%       2.749ms     916.261us           0 b         -16 b             3  \n                                         aten::_to_copy         0.03%      46.982us        11.14%      20.158ms       2.240ms           0 b           0 b             9  \n                                    aten::empty_strided         2.87%       5.187ms         2.88%       5.208ms     123.995us           8 b           8 b            42  \n                                            aten::copy_         0.08%     136.548us        11.16%      20.191ms       1.262ms           0 b           0 b            16  \n                                        cudaMemcpyAsync         0.07%     119.215us         0.07%     119.215us      11.922us           0 b           0 b            10  \n                                  cudaStreamSynchronize        11.00%      19.901ms        11.00%      19.901ms       4.975ms           0 b           0 b             4  \n                                           aten::expand         0.07%     125.291us         0.10%     173.842us       9.150us           0 b           0 b            19  \n                                             aten::view         0.02%      41.274us         0.02%      41.274us       2.948us           0 b           0 b            14  \n                                 aten::_index_put_impl_         0.03%      46.895us         0.10%     175.267us     175.267us           0 b           0 b             1  \n                                          aten::reshape         0.01%      16.294us         0.01%      26.400us       3.300us           0 b           0 b             8  \n                                            aten::slice         0.02%      29.612us         0.02%      31.011us      15.505us           0 b           0 b             2  \n                                        aten::transpose         0.01%      10.259us         0.01%      13.023us       6.512us           0 b           0 b             2  \n                                              aten::sum         0.03%      55.501us         0.06%     106.991us      53.495us           0 b           0 b             2  \n                                          aten::argsort         0.00%       3.241us         0.05%      88.042us      88.042us           0 b           0 b             1  \n                                             aten::sort         0.01%      22.313us         0.05%      84.801us      84.801us           0 b           0 b             1  \n                               TorchDynamo Cache Lookup         0.01%      18.885us         0.01%      18.885us      18.885us           0 b           0 b             1  \n                  _compile.compile_inner (dynamo_timed)        74.05%     133.976ms        80.87%     146.324ms     146.324ms           0 b         -16 b             1  \n                                  cudaStreamIsCapturing         0.01%      14.808us         0.01%      14.808us       1.645us           0 b           0 b             9  \n                                           aten::detach         0.07%     126.944us         0.11%     196.830us       5.320us           0 b           0 b            37  \n                                                 detach         0.04%      69.886us         0.04%      69.886us       4.111us           0 b           0 b            17  \n                                         aten::new_ones         0.15%     265.495us         0.18%     332.221us     166.111us           0 b           0 b             2  \n                                       aten::empty_like         0.13%     229.556us         2.41%       4.354ms       1.089ms          16 b           0 b             4  \n          OutputGraph.call_user_compiler (dynamo_timed)         0.09%     166.266us         0.09%     166.266us     166.266us           0 b           0 b             1  \n                                            aten::clone         0.01%      23.375us         0.40%     721.301us     240.434us          16 b           0 b             3  \n                                  Torch-Compiled Region         2.97%       5.373ms        15.67%      28.357ms      28.357ms           0 b         -28 b             1  \nautograd::engine::evaluate_function: torch::autograd...         0.00%       2.455us         0.00%       2.455us       2.455us           0 b           0 b             1  \n                                FlexAttentionAutogradOp         0.71%       1.287ms        12.28%      22.210ms      22.210ms           0 b          -4 b             1  \n                                aten::repeat_interleave         0.01%      25.332us         0.42%     768.125us     384.062us           0 b           0 b             2  \n                                             cudaMalloc         0.69%       1.255ms         0.69%       1.255ms     209.109us           0 b           0 b             6  \n                                          aten::flatten         0.01%      10.249us         0.01%      19.175us       9.587us           0 b           0 b             2  \n                                           aten::matmul         0.02%      39.080us         0.23%     422.852us     211.426us           0 b           0 b             2  \n                                   aten::_reshape_alias         0.00%       2.720us         0.00%       2.720us       2.720us           0 b           0 b             1  \n                                              aten::bmm         0.08%     136.645us         0.19%     343.996us     171.998us           0 b           0 b             2  \n          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.01%      17.791us         0.01%      17.791us       1.483us           0 b           0 b            12  \n                                     aten::_unsafe_view         0.00%       3.518us         0.00%       3.518us       1.759us           0 b           0 b             2  \n                                              aten::mul         0.02%      38.674us         0.14%     248.663us     248.663us           0 b           0 b             1  \n                                          aten::detach_         0.00%       0.587us         0.00%       0.587us       0.587us           0 b           0 b             1  \n                                        aten::logsumexp         0.04%      67.416us         0.55%     992.482us     496.241us           0 b           0 b             2  \n                                             aten::real         0.00%       0.538us         0.00%       0.538us       0.538us           0 b           0 b             1  \n                                             aten::amax         0.01%      23.702us         0.02%      32.499us      32.499us           0 b           0 b             1  \n                                          aten::squeeze         0.00%       3.883us         0.00%       5.472us       5.472us           0 b           0 b             1  \n                                              aten::abs         0.01%      18.143us         0.03%      54.137us      27.069us           0 b           0 b             2  \n                                               aten::eq         0.03%      60.341us         0.04%      78.529us      26.176us           0 b           0 b             3  \n                                     aten::masked_fill_         0.01%      12.441us         0.01%      18.692us      18.692us           0 b           0 b             1  \n                                              aten::sub         0.02%      27.975us         0.12%     219.036us     219.036us           0 b           0 b             1  \n                                             aten::exp_         0.01%      11.659us         0.01%      17.765us      17.765us           0 b           0 b             1  \n                                             aten::log_         0.00%       8.909us         0.01%      15.611us      15.611us           0 b           0 b             1  \n                                             aten::add_         0.00%       8.003us         0.01%      12.975us      12.975us           0 b           0 b             1  \n                                              aten::all         0.02%      35.610us         0.03%      51.602us      25.801us           0 b           0 b             2  \n                                    aten::_safe_softmax         0.01%      19.631us         0.20%     367.815us     367.815us           0 b           0 b             1  \n                                          aten::softmax         0.00%       4.041us         0.02%      27.929us      27.929us           0 b           0 b             1  \n                                         aten::_softmax         0.01%      16.418us         0.01%      23.888us      23.888us           0 b           0 b             1  \n                                              aten::div         0.01%      19.563us         0.01%      26.973us      26.973us           0 b           0 b             1  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 180.934ms\n\n=============== Benchmark Report: =========================\n<torch.utils.benchmark.utils.common.Measurement object at 0x7e5f32111570>\nMulti Threaded SDPA - flex_attention\nsetup: from __main__ import no_op\n  48.30 ms\n  1 measurement, 100 runs , 2 threads\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Part n-1 Completed!!\n\nWow. I was actually able to break down problem and attend to each of them individually, the next big task now is to find the \"goldiocks\" zone, and use a combination of approaches for which we have experimented. Before going there, if we are able to report only the important metrics from the `ProfileAnalysis.report()` function, we can reduce a lot of useless print statements. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}